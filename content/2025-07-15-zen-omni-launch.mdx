---
title: "Zen Omni: Multimodal AI for Text, Vision, and Audio"
date: "2025-07-15"
author: "Zach Kelling"
tags: ["ai", "models", "zen", "multimodal", "vision", "audio", "launch"]
description: "Zen Omni is a unified multimodal model handling text, images, and audio in a single pass — built for production applications that require cross-modal reasoning without model switching."
---

# Zen Omni: Multimodal AI for Text, Vision, and Audio

Today we are releasing Zen Omni, a unified multimodal model that processes text, images, and audio together.

Most multimodal AI pipelines string together separate models: one for speech transcription, one for vision, one for language. Zen Omni handles all three modalities in a single forward pass. There is no intermediate serialization, no context lost between modality handoffs, no latency penalty for model switching.

## What It Can Do

**Text + Vision**: Analyze diagrams, read documents with embedded charts, describe scenes, extract text from screenshots, reason about spatial relationships in images.

**Text + Audio**: Transcribe speech, understand spoken questions, respond to voice queries, identify speakers, classify sounds.

**Cross-modal reasoning**: Answer questions that require integrating information from an image and a spoken description simultaneously. Understand a whiteboard diagram while hearing a verbal explanation.

## Architecture

Zen Omni is built on the Zen MoDE (Mixture of Distilled Experts) architecture adapted for multimodal inputs. The model uses modality-specific encoders for vision and audio that project into a shared token space, which the language decoder then processes jointly.

| Property | Value |
|----------|-------|
| Parameters | ~30B (MoE) |
| Active Parameters | ~3B per forward pass |
| Architecture | Unified multimodal transformer |
| Text Context | 32,768 tokens |
| Image Resolution | Up to 4096×4096 tiles |
| Audio Input | Up to 30 seconds per clip |
| License | Apache 2.0 |

## Performance

| Task | Zen Omni | GPT-4o | Gemini 1.5 Flash |
|------|----------|--------|------------------|
| DocVQA | 94.1 | 91.1 | 89.7 |
| ChartQA | 88.3 | 85.7 | 85.5 |
| MathVista | 72.6 | 69.1 | 58.4 |
| Whisper WER (en) | 3.1% | 3.3% | 4.2% |
| AudioCaps CLAP | 0.81 | - | - |

DocVQA and ChartQA measure document and chart understanding. MathVista measures mathematical reasoning with visual context. WER is word error rate on speech transcription (lower is better). CLAP measures audio-caption alignment.

## Quickstart

```python
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import torch

processor = AutoProcessor.from_pretrained("zenlm/zen-omni")
model = AutoModelForCausalLM.from_pretrained(
    "zenlm/zen-omni",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Vision + text
image = Image.open("chart.png")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "What is the trend shown in this chart?"}
        ]
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=[image], return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=512)
print(processor.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True))
```

```python
# Audio + text
import librosa
audio, sr = librosa.load("question.wav", sr=16000)

messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": audio, "sampling_rate": sr},
            {"type": "text", "text": "Transcribe and answer the question in this audio."}
        ]
    }
]

inputs = processor(text=processor.apply_chat_template(messages, ...), audio=audio, ...)
output = model.generate(**inputs, max_new_tokens=256)
```

## Use Cases

**Document intelligence**: Process PDFs with embedded charts, tables, and figures. Ask questions about the content without extracting text first.

**Voice assistants**: Accept spoken input, reason with text, and produce spoken output -- in one model, with no transcription step leaking information.

**Screen understanding**: Analyze UI screenshots, identify elements, suggest interactions.

**Accessibility tooling**: Describe visual content for screen readers. Generate captions for audio. Translate between modalities.

**Research workflows**: Analyze scientific figures, read axis labels, understand experimental diagrams.

## Deployment

```bash
# vLLM (GPU)
vllm serve zenlm/zen-omni \
  --dtype bfloat16 \
  --max-model-len 32768 \
  --limit-mm-per-prompt image=5,audio=2

# Hanzo API
curl https://api.hanzo.ai/v1/chat/completions \
  -H "Authorization: Bearer $HANZO_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model": "zen-omni", "messages": [...]}'
```

## Get Zen Omni

- **HuggingFace**: [huggingface.co/zenlm/zen-omni](https://huggingface.co/zenlm/zen-omni)
- **Hanzo Cloud API**: Available at `api.hanzo.ai/v1/chat/completions` with model `zen-omni`
- **Zen LM**: [zenlm.org](https://zenlm.org) -- multimodal integration guides

---

*Zach Kelling is the founder of Hanzo AI, Techstars '17.*
