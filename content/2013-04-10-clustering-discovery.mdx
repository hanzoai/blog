---
title: "K-Means + Random Forest: Discovering Which Ad Combinations Actually Work"
date: "2013-04-10"
author: "Zach Kelling"
tags: ["machine-learning", "clustering", "random-forest", "advertising", "ai", "history"]
description: "We built a clustering system that algorithmically discovers high-performing ad creative combinations. Here's the method and what we're seeing in production."
---

# K-Means + Random Forest: Discovering Which Ad Combinations Actually Work

A typical campaign has thousands of possible creative combinations. Human teams test a handful of them and call it optimization. Most of the combination space never gets traffic, which means most of the potential performance gains are never discovered.

We've been building a system to fix this. Here's what we've shipped and what it's producing.

## The Problem with Manual A/B Testing

Standard A/B testing assumes you know what hypotheses to test. You pick two headline variants, measure click rate, pick a winner. This works when you're refining a configuration you already think is good.

It doesn't work for discovery — finding the configuration you haven't thought of yet. The combination space is too large for hypothesis-driven testing to cover meaningfully.

## Our Approach

We're applying two methods from the machine learning literature to the campaign data that's been accumulating on our platform since 2011.

**K-means clustering** groups similar campaign configurations by performance profile. Not by what they look like — by how they perform. Configurations that produce similar audience response patterns cluster together regardless of whether a human would have grouped them.

What we're finding: the high-performing cluster has structure. Certain creative combinations co-occur in it consistently. The cluster didn't know what the creative rules were, so it found patterns the rules would have filtered out.

**Random Forest feature importance** takes the clustered data and answers the question: which creative dimensions actually predict cluster membership? Which variables separate high-performing configurations from low-performing ones?

Running feature importance on our campaign corpus shows headline format and offer structure rank significantly above color scheme and font choice. This tells us where to allocate optimization effort. It also tells us what *not* to spend test traffic on.

## Production Results

First cohort: multi-item order rate up 18% against the manually optimized baseline. Statistical significance reached in 72 hours.

The system is finding combinations that our creative team wouldn't have proposed — pairings that violate aesthetic conventions but match audience behavior patterns the data has captured. The model doesn't have intuitions about what should work. It has evidence about what does.

## What We're Building Next

This clustering and importance analysis is the input to a genetic algorithm system (Earle) we're building to evolve campaign configurations rather than testing them discretely. Instead of starting from random combinations, Earle initializes populations in the high-importance regions of the configuration space the clustering analysis identifies.

The goal: converge on optimal configurations faster, discover non-obvious combinations, reduce the campaign traffic required to reach significance.

More on Earle when it ships. The clustering foundation is working well enough that we wanted to share the approach now.
