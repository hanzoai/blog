---
title: "Zen VL: Vision-Language Models at 4B, 8B, and 30B"
date: "2025-09-01"
author: "Zach Kelling"
tags: ["ai", "models", "zen", "vision", "multimodal", "vl", "launch"]
description: "Zen VL is a family of vision-language models at three scales — 4B, 8B, and 30B — with OCR across 32 languages, video understanding, and specialized agent variants for GUI interaction."
---

# Zen VL: Vision-Language Models at 4B, 8B, and 30B

Today we are releasing the Zen VL model family: six vision-language models at three parameter scales, each with instruct and agent variants.

Zen VL handles images, video, and documents. Every model in the family can describe scenes, answer visual questions, extract text via OCR, and reason about spatial relationships. The agent variants add function calling and GUI interaction -- enabling automated workflows that require visual context.

## The Family

| Model | Parameters | Context | Best For |
|-------|-----------|---------|----------|
| Zen VL 4B Instruct | 4B | 32K (→256K) | Edge devices, fast inference |
| Zen VL 4B Agent | 4B | 32K (→256K) | Automated visual workflows, small footprint |
| Zen VL 8B Instruct | 8B | 256K (→1M) | Balanced quality and speed |
| Zen VL 8B Agent | 8B | 256K (→1M) | Tool-use, GUI automation, coding with vision |
| Zen VL 30B Instruct | 30B | 256K (→1M) | High-accuracy visual analysis |
| Zen VL 30B Agent | 30B | 256K (→1M) | Complex agentic tasks with visual reasoning |

All six models are available on HuggingFace as SafeTensors (BF16).

## Capabilities

**Image understanding**: Scene description, object detection, spatial reasoning, attribute recognition, visual question answering.

**OCR**: Text extraction in 32 languages including Arabic, Chinese, Japanese, Korean, Hebrew, and 27 more. Table and form extraction. Mathematical formula recognition.

**Video**: Frame-level analysis, temporal reasoning, scene transition detection, activity recognition.

**Document AI**: PDF analysis, chart and graph interpretation, slide deck summarization, technical diagram understanding.

**Agent tasks** (agent variants only):
- GUI automation: Click, type, scroll actions for desktop and web
- Function calling: Invoke APIs based on visual context
- Multi-step reasoning: Decompose complex visual tasks into tool calls

## Performance

| Benchmark | Zen VL 8B | GPT-4V | Gemini Pro Vision |
|-----------|-----------|--------|-------------------|
| DocVQA | 91.3 | 87.2 | 86.5 |
| ChartQA | 85.6 | 78.5 | 74.3 |
| OCRBench | 82.1 | 74.1 | 71.2 |
| MMMU | 58.7 | 56.8 | 58.5 |
| ScreenSpot (GUI) | 86.3 | 51.2 | 48.7 |

ScreenSpot measures GUI grounding — the model's ability to locate UI elements on a screen given a text description. The 86.3% score on Zen VL 8B Agent reflects focused training on GUI interaction tasks.

## Quickstart

```python
from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
import torch

# Load 8B instruct model
processor = AutoProcessor.from_pretrained("zenlm/zen-vl-8b-instruct")
model = AutoModelForCausalLM.from_pretrained(
    "zenlm/zen-vl-8b-instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

image = Image.open("screenshot.png")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "Extract all text from this image, preserving structure."}
        ]
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=text, images=[image], return_tensors="pt").to(model.device)
output = model.generate(**inputs, max_new_tokens=1024)
response = processor.decode(output[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(response)
```

### Agent Variant with Tool Use

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "click_element",
            "description": "Click a UI element at the given screen coordinates",
            "parameters": {
                "type": "object",
                "properties": {
                    "x": {"type": "integer"},
                    "y": {"type": "integer"},
                    "element_description": {"type": "string"}
                },
                "required": ["x", "y"]
            }
        }
    }
]

# Agent variant supports function calling based on screen content
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": screenshot},
            {"type": "text", "text": "Click the Submit button."}
        ]
    }
]
text = processor.apply_chat_template(messages, tools=tools, ...)
```

## Extended Context

The 8B and 30B models support up to 1M token context via positional extrapolation. The 4B supports up to 256K. These extended lengths are useful for:

- Analyzing long video sequences frame by frame
- Processing multi-page PDF documents in a single pass
- Maintaining visual conversation history across many turns

For most use cases, 256K is more than sufficient. The base 32K/256K context works well for single images and short videos.

## Hardware

| Model | VRAM (BF16) | VRAM (Q4) | Tokens/sec (A100) |
|-------|------------|-----------|-------------------|
| Zen VL 4B | 8 GB | 3 GB | 145 |
| Zen VL 8B | 16 GB | 6 GB | 82 |
| Zen VL 30B | 64 GB | 20 GB | 31 |

30B models require multi-GPU on consumer hardware. 8B runs on a single 3090 or 4090 with BF16. 4B fits in 8GB VRAM with comfortable headroom for image tokens.

## Get Zen VL

- **HuggingFace**: [huggingface.co/zenlm](https://huggingface.co/zenlm) -- all six models
- **Hanzo Cloud API**: `zen-vl-8b` and `zen-vl-30b` available via API
- **Zen LM**: [zenlm.org](https://zenlm.org) -- benchmarks, deployment guides

---

*Zach Kelling is the founder of Hanzo AI, Techstars '17.*
