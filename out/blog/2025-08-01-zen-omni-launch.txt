1:"$Sreact.fragment"
2:I[2710,["933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-47e4a7ab857a36d7.js"],"ThemeProvider"]
3:I[9933,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Image"]
4:I[3977,[],""]
5:I[8765,[],""]
6:I[4526,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"FlickeringGrid"]
7:I[1964,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],""]
9:I[2463,[],"OutletBoundary"]
c:I[373,[],"AsyncMetadataOutlet"]
e:I[2463,[],"ViewportBoundary"]
10:I[2463,[],"MetadataBoundary"]
12:I[680,[],""]
:HL["/_next/static/media/27834908180db20f-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/78fec81b34c4a365.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/5ecf9fcd3bbfeb13.css","style"]
0:{"P":null,"b":"Szp5DJTNL0ohTsfq4PNfU","p":"","c":["","blog","2025-08-01-zen-omni-launch"],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","2025-08-01-zen-omni-launch","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5ecf9fcd3bbfeb13.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_245d8d __variable_97c177 antialiased","suppressHydrationWarning":true,"children":["$","body",null,{"children":["$","$L2",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","header",null,{"className":"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60","children":["$","div",null,{"className":"max-w-5xl mx-auto flex items-center justify-between","children":[["$","a",null,{"href":"https://hanzo.ai","className":"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity","children":[["$","$L3",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":20,"height":20,"className":"dark:invert-0 invert"}],["$","span",null,{"className":"font-semibold text-base tracking-tight","children":"hanzo"}],["$","span",null,{"className":"text-muted-foreground text-sm","children":"/ blog"}]]}],["$","nav",null,{"className":"flex items-center gap-4 text-sm text-muted-foreground","children":[["$","a",null,{"href":"https://hanzo.ai","className":"hover:text-foreground transition-colors","children":"hanzo.ai"}],["$","a",null,{"href":"https://hanzo.help","className":"hover:text-foreground transition-colors hidden sm:block","children":"Help"}],["$","a",null,{"href":"https://discord.gg/hanzo","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-message-circle h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","vv11sd",{"d":"M7.9 20A9 9 0 1 0 4 16.1L2 22Z"}],"$undefined"]}],"Discord"]}]]}]]}]}],["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"min-h-screen bg-background flex items-center justify-center w-full z-10","children":[["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L6",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.5,"flickerChance":0.1}]}],["$","div",null,{"className":"text-center flex flex-col gap-4 max-w-xs mx-auto relative","children":[["$","h1",null,{"className":"text-8xl font-mono font-bold drop-shadow-lg text-primary","children":"404"}],["$","p",null,{"className":"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance","children":"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL."}],["$","$L7",null,{"href":"/","children":"Back to Home","className":"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[>svg]:px-3 w-full rounded-lg h-9 drop-shadow-lg","ref":null}]]}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"border-t border-border/50 px-6 py-6 mt-auto","children":["$","div",null,{"className":"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground","children":[["$","div",null,{"className":"flex items-center gap-2","children":[["$","$L3",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":16,"height":16,"className":"dark:invert-0 invert opacity-50"}],["$","span",null,{"children":"Â© 2025 Hanzo AI, Inc. Techstars '17."}]]}],["$","div",null,{"className":"flex items-center gap-4","children":[["$","a",null,{"href":"https://hanzo.ai/privacy","className":"hover:text-foreground transition-colors","children":"Privacy"}],["$","a",null,{"href":"https://hanzo.ai/terms","className":"hover:text-foreground transition-colors","children":"Terms"}],["$","a",null,{"href":"https://hanzo.ai/contact","className":"hover:text-foreground transition-colors","children":"Contact"}],["$","a",null,{"href":"https://github.com/hanzoai","target":"_blank","rel":"noopener noreferrer","className":"hover:text-foreground transition-colors","children":"GitHub"}]]}]]}]}]]}]}]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","2025-08-01-zen-omni-launch","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L8",null,["$","$L9",null,{"children":["$La","$Lb",["$","$Lc",null,{"promise":"$@d"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Tgkdmh7jSakWbm8lvepHKv",{"children":[["$","$Le",null,{"children":"$Lf"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$L10",null,{"children":"$L11"}]]}],false]],"m":"$undefined","G":["$12","$undefined"],"s":false,"S":true}
13:"$Sreact.suspense"
14:I[373,[],"AsyncMetadata"]
16:I[9048,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"HashScrollHandler"]
17:I[4170,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CopyHeader"]
18:I[3979,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"default"]
19:I[9372,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"TableOfContents"]
1a:I[1413,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"MobileTableOfContents"]
11:["$","div",null,{"hidden":true,"children":["$","$13",null,{"fallback":null,"children":["$","$L14",null,{"promise":"$@15"}]}]}]
8:["$","div",null,{"className":"min-h-screen bg-background relative","children":[["$","$L16",null,{}],["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L6",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.2,"flickerChance":0.05}]}],["$","div",null,{"className":"space-y-4 border-b border-border relative z-10","children":["$","div",null,{"className":"max-w-7xl mx-auto flex flex-col gap-6 p-6","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground","children":[["$","$L7",null,{"href":"/","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left w-4 h-4","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"Back to all articles"}]],"className":"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 h-6 w-6","ref":null}],["$","div",null,{"className":"flex flex-wrap gap-3 text-muted-foreground","children":[["$","span","ai",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"ai"}],["$","span","models",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"models"}],["$","span","zen",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen"}],["$","span","multimodal",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"multimodal"}],["$","span","audio",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"audio"}],["$","span","speech",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"speech"}],["$","span","launch",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"launch"}],["$","span","zen-mode",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen-mode"}]]}],["$","time",null,{"className":"font-medium text-muted-foreground","children":"July 31, 2025"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance","children":"Zen Omni: Unified Multimodal AI"}],["$","p",null,{"className":"text-muted-foreground max-w-4xl md:text-lg md:text-balance","children":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}]]}]}],["$","div",null,{"className":"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10","children":[["$","div",null,{"className":"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"}],["$","main",null,{"className":"w-full p-0 overflow-hidden","children":[null,["$","div",null,{"className":"p-6 lg:p-10","children":["$","div",null,{"className":"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg","children":["$","div",null,{"ref":"$undefined","children":[["$","$L17",null,{"level":1,"id":"zen-omni-unified-multimodal-ai","children":"Zen Omni: Unified Multimodal AI"}],"\n",["$","p",null,{"children":"Zen Omni is a 30B MoE model that handles text, vision, and audio in a single unified model. The architecture is called Thinker-Talker: a shared reasoning backbone that branches into modality-specific output heads for text and speech generation."}],"\n",["$","p",null,{"children":"Active parameters: 3B per forward pass. Sub-300ms speech-to-speech latency."}],"\n",["$","$L17",null,{"level":2,"id":"thinker-talker-architecture","children":"Thinker-Talker Architecture"}],"\n",["$","p",null,{"children":"Most multimodal systems chain separate models: a speech recognizer feeds into a language model that feeds into a text-to-speech engine. Each handoff adds latency, loses context, and introduces a seam where errors compound."}],"\n",["$","p",null,{"children":"Zen Omni eliminates the handoffs. The Thinker is a shared MoE backbone that processes all modalities in a unified token space. Text tokens, image tokens, and audio tokens flow through the same attention layers. The Talker is a small output head attached to the Thinker that generates speech directly from the same hidden states that produce text."}],"\n",["$","p",null,{"children":"This means:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"The model hears your voice and understands your intent directly, without a transcription step"}],"\n",["$","li",null,{"children":"It speaks its response without converting text to audio after the fact"}],"\n",["$","li",null,{"children":"Emotional tone and prosody in input speech influence the generated response"}],"\n",["$","li",null,{"children":"Visual context informs both text and speech outputs simultaneously"}],"\n"]}],"\n",["$","$L17",null,{"level":2,"id":"specifications","children":"Specifications"}],"\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Property"}],["$","th",null,{"children":"Value"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Total parameters"}],["$","td",null,{"children":"30B"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Active parameters"}],["$","td",null,{"children":"3B"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Architecture"}],["$","td",null,{"children":"MoE (Thinker-Talker)"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Text context"}],["$","td",null,{"children":"128K tokens"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Audio context"}],["$","td",null,{"children":"10 minutes"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Speech-to-speech latency"}],["$","td",null,{"children":"<300ms"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Languages (text)"}],["$","td",null,{"children":"100+"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Languages (speech)"}],["$","td",null,{"children":"58"}]]}]]}]]}]}],"\n",["$","$L17",null,{"level":2,"id":"real-time-speech-to-speech","children":"Real-Time Speech-to-Speech"}],"\n",["$","p",null,{"children":"Under 300ms latency for speech-to-speech exchange makes Zen Omni suitable for real-time voice applications:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Voice assistants with natural conversational rhythm"}],"\n",["$","li",null,{"children":"Real-time voice translation (speech in, speech out, same or different language)"}],"\n",["$","li",null,{"children":"Interactive voice response (IVR) with genuine language understanding"}],"\n",["$","li",null,{"children":"Accessibility tools for vision-impaired users"}],"\n"]}],"\n",["$","p",null,{"children":"300ms is below the conversational latency threshold where pauses become uncomfortable. It is achievable because the Talker generates speech tokens in parallel with the Thinker's reasoning, not sequentially after."}],"\n",["$","$L17",null,{"level":2,"id":"zen-dub-integration","children":"Zen Dub Integration"}],"\n",["$","p",null,{"children":["Zen Omni integrates with ",["$","$L18",null,{"href":"https://zenlm.org/dub","children":"Zen Dub"}],", our voice cloning and audio generation model. When Zen Dub is paired with Zen Omni, the speech output can be delivered in a cloned voice rather than the default model voice."]}],"\n",["$","p",null,{"children":"Use cases:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Branded AI assistants with a consistent voice identity"}],"\n",["$","li",null,{"children":"Audio content localization that preserves the original speaker's voice"}],"\n",["$","li",null,{"children":"Personalized voice interfaces"}],"\n"]}],"\n",["$","$L17",null,{"level":2,"id":"input-and-output-modalities","children":"Input and Output Modalities"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Input"}],": Text, images, video (up to 5 minutes), audio (up to 10 minutes), interleaved multimodal sequences"]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Output"}],": Text, speech audio, structured data"]}],"\n",["$","p",null,{"children":"The model handles interleaved inputs naturally: a conversation can include text messages, attached images, voice memos, and video clips, all in a single context window. The model understands relationships across all of them."}],"\n",["$","$L17",null,{"level":2,"id":"get-zen-omni","children":"Get Zen Omni"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"HuggingFace"}],": ",["$","$L18",null,{"href":"https://huggingface.co/zenlm","children":"huggingface.co/zenlm"}]]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Hanzo Cloud API"}],": ",["$","code",null,{"children":"api.hanzo.ai/v1/chat/completions"}]," -- model ",["$","code",null,{"children":"zen-omni"}],", voice output via ",["$","code",null,{"children":"audio"}]," response format"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Zen LM"}],": ",["$","$L18",null,{"href":"https://zenlm.org","children":"zenlm.org"}]," -- audio API documentation"]}],"\n"]}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Zach Kelling is the founder of Hanzo AI, Techstars '17."}]}]],"className":"prose"}]}]}],["$","div",null,{"className":"mt-10","children":["$","section",null,{"className":"border-t border-border p-0","children":["$","div",null,{"className":"p-6 lg:p-10","children":[["$","h2",null,{"className":"text-2xl font-medium mb-8","children":"Read more"}],["$","div",null,{"className":"flex flex-col gap-8","children":[["$","$L7","/blog/2025-07-01-zen-designer-launch",{"href":"/blog/2025-07-01-zen-designer-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Designer: 235B Vision-Language Model"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"June 30, 2025"}]]}]]}],["$","$L7","/blog/2024-07-20-zen-vision-launch",{"href":"/blog/2024-07-20-zen-vision-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"July 19, 2024"}]]}]]}],["$","$L7","/blog/2025-10-10-zen-audit-launch",{"href":"/blog/2025-10-10-zen-audit-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Audit: Code Security and Smart Contract Analysis"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Audit is trained on CVE databases, audit reports, and vulnerability research to provide automated code security analysis and smart contract auditing with low false positive rates."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"October 9, 2025"}]]}]]}]]}]]}]}]}]]}],["$","aside",null,{"className":"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20","children":["$","div",null,{"className":"sticky top-20 space-y-8","children":[["$","div",null,{"className":"flex items-start gap-2","children":["$","div",null,{"className":"flex-1","children":["$","h3",null,{"className":"text-sm tracking-tight text-balance font-semibold","children":"Zach Kelling"}]}]}],["$","div",null,{"className":"border border-border rounded-lg p-6 bg-card","children":["$","$L19",null,{}]}]]}]}]]}],["$","$L1a",null,{}]]}]
b:null
f:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","content":"black"}]]
a:null
d:{"metadata":[["$","title","0",{"children":"Zen Omni: Unified Multimodal AI - Hanzo Blog"}],["$","meta","1",{"name":"description","content":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}],["$","link","2",{"rel":"author","href":"https://blog.hanzo.ai"}],["$","meta","3",{"name":"author","content":"Zach Kelling"}],["$","meta","4",{"name":"keywords","content":"Zen Omni: Unified Multimodal AI,ai,models,zen,multimodal,audio,speech,launch,zen-mode,Hanzo AI,Blog,AI Research,Infrastructure"}],["$","meta","5",{"name":"creator","content":"Zach Kelling"}],["$","meta","6",{"name":"publisher","content":"Hanzo AI"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://blog.hanzo.ai/blog/2025-08-01-zen-omni-launch"}],["$","meta","10",{"property":"og:title","content":"Zen Omni: Unified Multimodal AI"}],["$","meta","11",{"property":"og:description","content":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}],["$","meta","12",{"property":"og:url","content":"https://blog.hanzo.ai/blog/2025-08-01-zen-omni-launch"}],["$","meta","13",{"property":"og:site_name","content":"Hanzo Blog"}],["$","meta","14",{"property":"og:image","content":"https://blog.hanzo.ai/blog/2025-08-01-zen-omni-launch/opengraph-image"}],["$","meta","15",{"property":"og:image:width","content":"1200"}],["$","meta","16",{"property":"og:image:height","content":"630"}],["$","meta","17",{"property":"og:image:alt","content":"Zen Omni: Unified Multimodal AI"}],["$","meta","18",{"property":"og:type","content":"article"}],["$","meta","19",{"property":"article:published_time","content":"2025-08-01"}],["$","meta","20",{"property":"article:author","content":"Zach Kelling"}],["$","meta","21",{"property":"article:tag","content":"ai"}],["$","meta","22",{"property":"article:tag","content":"models"}],["$","meta","23",{"property":"article:tag","content":"zen"}],["$","meta","24",{"property":"article:tag","content":"multimodal"}],["$","meta","25",{"property":"article:tag","content":"audio"}],["$","meta","26",{"property":"article:tag","content":"speech"}],["$","meta","27",{"property":"article:tag","content":"launch"}],["$","meta","28",{"property":"article:tag","content":"zen-mode"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:site","content":"@hanzoai"}],["$","meta","31",{"name":"twitter:creator","content":"@hanzoai"}],["$","meta","32",{"name":"twitter:title","content":"Zen Omni: Unified Multimodal AI"}],["$","meta","33",{"name":"twitter:description","content":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}],["$","meta","34",{"name":"twitter:image","content":"https://blog.hanzo.ai/blog/2025-08-01-zen-omni-launch/opengraph-image"}]],"error":null,"digest":"$undefined"}
15:{"metadata":"$d:metadata","error":null,"digest":"$undefined"}
