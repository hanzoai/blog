<!DOCTYPE html><html lang="en" class="__variable_245d8d __variable_97c177 antialiased"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/27834908180db20f-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/78fec81b34c4a365.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/45441016e214994e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-19293e76a0797e5c.js"/><script src="/_next/static/chunks/8b8ad143-e83b7b49f5b24ad0.js" async=""></script><script src="/_next/static/chunks/766-52c0c652dbe24189.js" async=""></script><script src="/_next/static/chunks/main-app-e8025b732b0b8d44.js" async=""></script><script src="/_next/static/chunks/cd24890f-00c67ea9d0bbf445.js" async=""></script><script src="/_next/static/chunks/933-166e0fcc9496c98f.js" async=""></script><script src="/_next/static/chunks/app/layout-d687a3a56998d23e.js" async=""></script><script src="/_next/static/chunks/771-2096a43097a1ac56.js" async=""></script><script src="/_next/static/chunks/698-980099c394b58d6f.js" async=""></script><script src="/_next/static/chunks/218-3bb2d990159730f8.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js" async=""></script><meta name="next-size-adjust" content=""/><meta name="theme-color" content="black"/><title>2.8 Billion Tokens Per Month: LLM Gateway at Production Scale - Hanzo Blog</title><meta name="description" content="The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here&#x27;s how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."/><link rel="author" href="https://hanzo.blog"/><meta name="author" content="Hanzo AI"/><meta name="keywords" content="2.8 Billion Tokens Per Month: LLM Gateway at Production Scale,infrastructure,llm,gateway,production,ai,Hanzo AI,Blog,AI Research,Infrastructure"/><meta name="creator" content="Hanzo AI"/><meta name="publisher" content="Hanzo AI"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale"/><meta property="og:title" content="2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"/><meta property="og:description" content="The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here&#x27;s how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."/><meta property="og:url" content="https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale"/><meta property="og:site_name" content="Hanzo Blog"/><meta property="og:image" content="https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2026-02-10"/><meta property="article:author" content="Hanzo AI"/><meta property="article:tag" content="infrastructure"/><meta property="article:tag" content="llm"/><meta property="article:tag" content="gateway"/><meta property="article:tag" content="production"/><meta property="article:tag" content="ai"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@hanzoai"/><meta name="twitter:creator" content="@hanzoai"/><meta name="twitter:title" content="2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"/><meta name="twitter:description" content="The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here&#x27;s how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."/><meta name="twitter:image" content="https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><script>((e,t,r,n,i,o,a,s)=>{let l=document.documentElement,u=["light","dark"];function c(t){var r;(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?i.map(e=>o[e]||e):i;r?(l.classList.remove(...n),l.classList.add(o&&o[t]?o[t]:t)):l.setAttribute(e,t)}),r=t,s&&u.includes(r)&&(l.style.colorScheme=r)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","dark",null,["light","dark"],null,true,true)</script><header class="border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60"><div class="max-w-5xl mx-auto flex items-center justify-between"><a href="https://hanzo.ai" class="flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity"><img alt="Hanzo" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="dark:invert-0 invert" style="color:transparent" src="/hanzo-logo.svg"/><span class="font-semibold text-base tracking-tight">hanzo</span><span class="text-muted-foreground text-sm">/ blog</span></a><nav class="flex items-center gap-4 text-sm text-muted-foreground"><a href="https://hanzo.ai" class="hover:text-foreground transition-colors">hanzo.ai</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay</a><a href="https://discord.gg/hanzo" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle h-3.5 w-3.5" aria-hidden="true"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg>Discord</a></nav></div></header><div class="min-h-screen bg-background relative"><div class="absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]"><div class="absolute top-0 left-0 size-full"><canvas class="pointer-events-none" style="width:0;height:0"></canvas></div></div><div class="space-y-4 border-b border-border relative z-10"><div class="max-w-7xl mx-auto flex flex-col gap-6 p-6"><div class="flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground"><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[&gt;svg]:px-3 h-6 w-6" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg><span class="sr-only">Back to all articles</span></a><div class="flex flex-wrap gap-3 text-muted-foreground"><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">infrastructure</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">llm</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">gateway</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">production</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">ai</span></div><time class="font-medium text-muted-foreground">February 9, 2026</time></div><h1 class="text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance">2.8 Billion Tokens Per Month: LLM Gateway at Production Scale</h1><p class="text-muted-foreground max-w-4xl md:text-lg md:text-balance">The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here&#x27;s how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production.</p></div></div><div class="flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10"><div class="absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"></div><main class="w-full p-0 overflow-hidden"><div class="p-6 lg:p-10"><div class="prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg"><div class="prose"><h1 id="28-billion-tokens-per-month-llm-gateway-at-production-scale" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">2.8 Billion Tokens Per Month: LLM Gateway at Production Scale<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Most teams reach for a single AI provider. They hardcode an API key, ship to production, and discover the failure modes incrementally: provider outages, rate limits, latency spikes, cost surprises, and the organizational friction of switching providers when a better model ships.</p>
<p>The Hanzo LLM Gateway was built to eliminate those failure modes. Today it processes 2.8 billion tokens per month across 100+ providers at 99.97% availability. Here is how it actually works.</p>
<h2 id="what-9997-means" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">What 99.97% Means<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>99.97% availability is approximately 2.6 hours of downtime per year. For a system routing to 100+ third-party providers — each with their own availability characteristics, rate limits, and deployment schedules — this number does not happen by accident.</p>
<p>The gateway achieves it through layered redundancy. No single provider failure causes gateway unavailability. Failover chains are configured per-model-class: if provider A returns a 503, the request routes to provider B with the semantically closest model at comparable price. If provider B also fails, it routes to provider C. The fallback chain is evaluated in milliseconds; the user sees elevated latency, not an error.</p>
<p>Streaming responses require special handling in failover. A streaming request that begins successfully from provider A cannot be transparently rerouted mid-stream if provider A drops the connection. The gateway&#x27;s streaming-aware load balancer tracks active stream health and executes a clean restart with the retry provider when a stream fails before the first token is delivered. Mid-stream failures surface as errors — this is honest rather than attempting a seamless splice that would corrupt output.</p>
<h2 id="semantic-caching" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Semantic Caching<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The 34% cache hit rate is the most operationally significant number after availability. A cache hit means the response is served in under 2 milliseconds without touching any AI provider. For the approximately one-third of requests that hit cache, the effective cost is near zero.</p>
<p>Semantic caching differs from exact-match caching. Two prompts that ask the same question with different phrasing — &quot;translate this to French&quot; versus &quot;convert this text to French&quot; — are semantically equivalent. An exact-match cache misses this; a semantic cache catches it.</p>
<p>The implementation uses embedding similarity over a vector index of cached prompt-response pairs. An incoming prompt is embedded; if the nearest cached embedding is within the similarity threshold, the cached response is returned. The threshold is configurable per model class — lower threshold for factual Q&amp;A (where small prompt changes may produce importantly different answers), higher threshold for classification tasks (where semantically similar prompts reliably have the same answer).</p>
<p>Cache invalidation is TTL-based per model. Models that update frequently (rolling deployments of fine-tuned models) use short TTLs. Stable models use longer TTLs. The 34% hit rate is the aggregate across all model classes and TTL configurations.</p>
<h2 id="streaming-aware-load-balancing" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Streaming-Aware Load Balancing<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The load balancer tracks provider health per model endpoint, not per provider globally. A provider can be degraded for a specific model while healthy for others. Health signals are:</p>
<ul>
<li>Response latency (p50, p95, p99 over a rolling window)</li>
<li>Error rate (4xx and 5xx, weighted by recency)</li>
<li>Token delivery rate (tokens per second for streaming responses)</li>
<li>Rate limit proximity (estimated from response headers where exposed)</li>
</ul>
<p>The load balancer uses these signals to route new requests toward providers with favorable health profiles. It does not use round-robin or static weights; routing is dynamic and updates on every health event.</p>
<p>The 23% latency reduction relative to single-provider routing comes from this dynamic selection. The best provider at a given moment — in terms of current queue depth, token delivery rate, and time-to-first-token — varies continuously. Static routing picks a provider and stays there through its bad moments. Dynamic routing moves.</p>
<h2 id="cost-attribution-at-scale" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Cost Attribution at Scale<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>2.8 billion tokens per month across 100+ providers means 100+ different pricing models. Per-input-token, per-output-token, per-request, per-image, per-minute of audio — each provider invoices differently. The gateway normalizes all of this into a per-token attribution that gives downstream cost reporting a consistent unit of account.</p>
<p>The attribution system tracks:</p>
<ul>
<li>Raw tokens (input and output separately, since they price differently at every provider)</li>
<li>Model identifier and provider</li>
<li>Request metadata (team, project, API key)</li>
<li>Wall-clock time for latency attribution</li>
</ul>
<p>Cost reports are available per API key, per project, and per team through the console. The 31% cost reduction relative to unmanaged single-provider usage comes from a combination of semantic caching (cached requests have near-zero cost), dynamic routing toward price-efficient providers for equivalent quality, and visibility into cost that allows teams to make informed tradeoffs.</p>
<h2 id="the-unified-api-surface" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">The Unified API Surface<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Every capability routes through a single OpenAI-compatible endpoint. Chat completions, embeddings, reranking, image generation, audio transcription, and content moderation all use the same authentication, the same SDKs, and the same cost attribution pipeline. Adding a new provider means registering its adapter in the gateway; consumer code does not change.</p>
<p>This composability was the original design goal. The production metrics — 2.8B tokens/month, 99.97% availability, 34% cache hit rate, 23% latency reduction, 31% cost reduction — are the measured outcomes of running that design at scale.</p>
<figure dir="ltr" class="my-4 rounded-xl bg-fd-card p-1 shiki relative border outline-none not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="0"><div class="empty:hidden absolute top-1 right-1 z-2 bg-fd-card rounded-bl-lg border-l border-b text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md p-2 text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none hover:bg-fd-accent hover:text-fd-accent-foreground [&amp;_svg]:size-3.5" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div><div class="bg-fd-secondary rounded-lg border text-[13px] py-3.5 overflow-auto max-h-[600px] fd-scroll-container" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"># Same code, different model, different provider:</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> https://llm.hanzo.ai/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &quot;Authorization: Bearer </span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">$HANZO_API_KEY</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">  -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &#x27;{&quot;model&quot;: &quot;zen4&quot;, &quot;messages&quot;: [...]}&#x27;</span></span></code></pre></div></figure>
<p>The gateway is available at <a href="https://llm.hanzo.ai" rel="noreferrer noopener" target="_blank">llm.hanzo.ai</a> and the source is open at <a href="https://github.com/hanzoai/llm" rel="noreferrer noopener" target="_blank">github.com/hanzoai/llm</a>.</p></div></div></div><div class="mt-10"><section class="border-t border-border p-0"><div class="p-6 lg:p-10"><h2 class="text-2xl font-medium mb-8">Read more</h2><div class="flex flex-col gap-8"><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2024-03-12-llm-infrastructure"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">LLM Infrastructure: Running AI at Scale</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">How we built infrastructure to serve billions of LLM requests for commerce applications.</p><time class="block text-xs font-medium text-muted-foreground">March 11, 2024</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2026-02-27-full-stack-ai-agents"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">The Complete AI Agent Stack: Models, Compute, and Tools in One Platform</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Hanzo AI introduces the first platform combining 100+ AI models, cloud compute, GPU access, and 260+ MCP tools under a single developer account.</p><time class="block text-xs font-medium text-muted-foreground">February 26, 2026</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2026-02-27-unified-ai-gateway"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">One API for Every AI Model: Introducing the Hanzo AI Gateway</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Hanzo AI launches the industry&#x27;s first zero-markup multi-provider AI gateway — one API key for 100+ models from every major provider, plus 14 proprietary Zen models.</p><time class="block text-xs font-medium text-muted-foreground">February 26, 2026</time></div></a></div></div></section></div></main><aside class="hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20"><div class="sticky top-20 space-y-8"><div class="flex items-start gap-2"><div class="flex-1"><h3 class="text-sm tracking-tight text-balance font-semibold">Hanzo AI</h3></div></div><div class="border border-border rounded-lg p-6 bg-card"></div></div></aside></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-«R2nnelb»" data-state="closed" class="lg:hidden fixed bottom-6 right-6 z-50 bg-primary text-primary-foreground p-3 rounded-full shadow-lg hover:bg-primary/90 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-list" aria-hidden="true"><path d="M3 12h.01"></path><path d="M3 18h.01"></path><path d="M3 6h.01"></path><path d="M8 12h13"></path><path d="M8 18h13"></path><path d="M8 6h13"></path></svg></button></div><!--$--><!--/$--><footer class="border-t border-border/50 px-6 py-6 mt-auto"><div class="max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground"><div class="flex items-center gap-2"><img alt="Hanzo" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" class="dark:invert-0 invert opacity-50" style="color:transparent" src="/hanzo-logo.svg"/><span>© 2025 Hanzo AI, Inc. Techstars &#x27;17.</span></div><div class="flex items-center gap-4"><a href="https://hanzo.ai/privacy" class="hover:text-foreground transition-colors">Privacy</a><a href="https://hanzo.ai/terms" class="hover:text-foreground transition-colors">Terms</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo blog</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen blog</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay.blog</a><a href="https://github.com/hanzoai" target="_blank" rel="noopener noreferrer" class="hover:text-foreground transition-colors">GitHub</a></div></div></footer><script src="/_next/static/chunks/webpack-19293e76a0797e5c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[861,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"PostHogProvider\"]\n3:I[2710,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"ThemeProvider\"]\n4:I[9933,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"Image\"]\n5:I[3977,[],\"\"]\n6:I[8765,[],\"\"]\n7:I[4526,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"FlickeringGrid\"]\n8:I[1964,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"\"]\na:I[2463,[],\"OutletBoundary\"]\nd:I[373,[],\"AsyncMetadataOutlet\"]\nf:I[2463,[],\"ViewportBoundary\"]\n11:I[2463,[],\"MetadataBoundary\"]\n13:I[680,[],\"\"]\n:HL[\"/_next/static/media/27834908180db20f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/78fec81b34c4a365.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/45441016e214994e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yeRLKKIdrV-2cpcPU6Jw-\",\"p\":\"\",\"c\":[\"\",\"blog\",\"2026-02-10-llm-gateway-production-scale\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2026-02-10-llm-gateway-production-scale\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/45441016e214994e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_245d8d __variable_97c177 antialiased\",\"suppressHydrationWarning\":true,\"children\":[\"$undefined\",[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"attribute\":\"class\",\"defaultTheme\":\"dark\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[[\"$\",\"header\",null,{\"className\":\"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex items-center justify-between\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":20,\"height\":20,\"className\":\"dark:invert-0 invert\"}],[\"$\",\"span\",null,{\"className\":\"font-semibold text-base tracking-tight\",\"children\":\"hanzo\"}],[\"$\",\"span\",null,{\"className\":\"text-muted-foreground text-sm\",\"children\":\"/ blog\"}]]}],[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"hanzo.ai\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay\"}],[\"$\",\"a\",null,{\"href\":\"https://discord.gg/hanzo\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-message-circle h-3.5 w-3.5\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"vv11sd\",{\"d\":\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"}],\"$undefined\"]}],\"Discord\"]}]]}]]}]}],[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background flex items-center justify-center w-full z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.5,\"flickerChance\":0.1}]}],[\"$\",\"div\",null,{\"className\":\"text-center flex flex-col gap-4 max-w-xs mx-auto relative\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-8xl font-mono font-bold drop-shadow-lg text-primary\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance\",\"children\":\"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL.\"}],[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":\"Back to Home\",\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[\u003esvg]:px-3 w-full rounded-lg h-9 drop-shadow-lg\",\"ref\":null}]]}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"className\":\"border-t border-border/50 px-6 py-6 mt-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":16,\"height\":16,\"className\":\"dark:invert-0 invert opacity-50\"}],[\"$\",\"span\",null,{\"children\":\"© 2025 Hanzo AI, Inc. Techstars '17.\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/privacy\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Privacy\"}],[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/terms\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Terms\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay.blog\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/hanzoai\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"GitHub\"}]]}]]}]}]]}]}]}]]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"2026-02-10-llm-gateway-production-scale\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L9\",null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"JRFEaAmQYAuiy140IxNYJv\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$L11\",null,{\"children\":\"$L12\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"14:\"$Sreact.suspense\"\n15:I[373,[],\"AsyncMetadata\"]\n17:I[9048,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"HashScrollHandler\"]\n18:I[4170,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"CopyHeader\"]\n19:I[4255,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"CodeBlock\"]\n1a:I[4255,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"Pre\"]\n1b:I[3979,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"default\"]\n1c:I[9372,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"TableOfContents\"]\n1d:I[1413,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"MobileTableOfContents\"]\n12:[\"$\",\"div\",null,{\"hidden\":true,\"children\":["])</script><script>self.__next_f.push([1,"\"$\",\"$14\",null,{\"fallback\":null,\"children\":[\"$\",\"$L15\",null,{\"promise\":\"$@16\"}]}]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background relative\",\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.2,\"flickerChance\":0.05}]}],[\"$\",\"div\",null,{\"className\":\"space-y-4 border-b border-border relative z-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto flex flex-col gap-6 p-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground\",\"children\":[[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left w-4 h-4\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Back to all articles\"}]],\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[\u003esvg]:px-3 h-6 w-6\",\"ref\":null}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 text-muted-foreground\",\"children\":[[\"$\",\"span\",\"infrastructure\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"infrastructure\"}],[\"$\",\"span\",\"llm\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"llm\"}],[\"$\",\"span\",\"gateway\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"gateway\"}],[\"$\",\"span\",\"production\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"production\"}],[\"$\",\"span\",\"ai\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"ai\"}]]}],[\"$\",\"time\",null,{\"className\":\"font-medium text-muted-foreground\",\"children\":\"February 9, 2026\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance\",\"children\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground max-w-4xl md:text-lg md:text-balance\",\"children\":\"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none\"}],[\"$\",\"main\",null,{\"className\":\"w-full p-0 overflow-hidden\",\"children\":[null,[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg\",\"children\":[\"$\",\"div\",null,{\"ref\":\"$undefined\",\"children\":[[\"$\",\"$L18\",null,{\"level\":1,\"id\":\"28-billion-tokens-per-month-llm-gateway-at-production-scale\",\"children\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Most teams reach for a single AI provider. They hardcode an API key, ship to production, and discover the failure modes incrementally: provider outages, rate limits, latency spikes, cost surprises, and the organizational friction of switching providers when a better model ships.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The Hanzo LLM Gateway was built to eliminate those failure modes. Today it processes 2.8 billion tokens per month across 100+ providers at 99.97% availability. Here is how it actually works.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"what-9997-means\",\"children\":\"What 99.97% Means\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"99.97% availability is approximately 2.6 hours of downtime per year. For a system routing to 100+ third-party providers — each with their own availability characteristics, rate limits, and deployment schedules — this number does not happen by accident.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The gateway achieves it through layered redundancy. No single provider failure causes gateway unavailability. Failover chains are configured per-model-class: if provider A returns a 503, the request routes to provider B with the semantically closest model at comparable price. If provider B also fails, it routes to provider C. The fallback chain is evaluated in milliseconds; the user sees elevated latency, not an error.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Streaming responses require special handling in failover. A streaming request that begins successfully from provider A cannot be transparently rerouted mid-stream if provider A drops the connection. The gateway's streaming-aware load balancer tracks active stream health and executes a clean restart with the retry provider when a stream fails before the first token is delivered. Mid-stream failures surface as errors — this is honest rather than attempting a seamless splice that would corrupt output.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"semantic-caching\",\"children\":\"Semantic Caching\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The 34% cache hit rate is the most operationally significant number after availability. A cache hit means the response is served in under 2 milliseconds without touching any AI provider. For the approximately one-third of requests that hit cache, the effective cost is near zero.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Semantic caching differs from exact-match caching. Two prompts that ask the same question with different phrasing — \\\"translate this to French\\\" versus \\\"convert this text to French\\\" — are semantically equivalent. An exact-match cache misses this; a semantic cache catches it.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The implementation uses embedding similarity over a vector index of cached prompt-response pairs. An incoming prompt is embedded; if the nearest cached embedding is within the similarity threshold, the cached response is returned. The threshold is configurable per model class — lower threshold for factual Q\u0026A (where small prompt changes may produce importantly different answers), higher threshold for classification tasks (where semantically similar prompts reliably have the same answer).\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Cache invalidation is TTL-based per model. Models that update frequently (rolling deployments of fine-tuned models) use short TTLs. Stable models use longer TTLs. The 34% hit rate is the aggregate across all model classes and TTL configurations.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"streaming-aware-load-balancing\",\"children\":\"Streaming-Aware Load Balancing\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The load balancer tracks provider health per model endpoint, not per provider globally. A provider can be degraded for a specific model while healthy for others. Health signals are:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Response latency (p50, p95, p99 over a rolling window)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Error rate (4xx and 5xx, weighted by recency)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Token delivery rate (tokens per second for streaming responses)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Rate limit proximity (estimated from response headers where exposed)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The load balancer uses these signals to route new requests toward providers with favorable health profiles. It does not use round-robin or static weights; routing is dynamic and updates on every health event.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The 23% latency reduction relative to single-provider routing comes from this dynamic selection. The best provider at a given moment — in terms of current queue depth, token delivery rate, and time-to-first-token — varies continuously. Static routing picks a provider and stays there through its bad moments. Dynamic routing moves.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"cost-attribution-at-scale\",\"children\":\"Cost Attribution at Scale\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"2.8 billion tokens per month across 100+ providers means 100+ different pricing models. Per-input-token, per-output-token, per-request, per-image, per-minute of audio — each provider invoices differently. The gateway normalizes all of this into a per-token attribution that gives downstream cost reporting a consistent unit of account.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The attribution system tracks:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Raw tokens (input and output separately, since they price differently at every provider)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Model identifier and provider\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Request metadata (team, project, API key)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Wall-clock time for latency attribution\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Cost reports are available per API key, per project, and per team through the console. The 31% cost reduction relative to unmanaged single-provider usage comes from a combination of semantic caching (cached requests have near-zero cost), dynamic routing toward price-efficient providers for equivalent quality, and visibility into cost that allows teams to make informed tradeoffs.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"the-unified-api-surface\",\"children\":\"The Unified API Surface\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Every capability routes through a single OpenAI-compatible endpoint. Chat completions, embeddings, reranking, image generation, audio transcription, and content moderation all use the same authentication, the same SDKs, and the same cost attribution pipeline. Adding a new provider means registering its adapter in the gateway; consumer code does not change.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This composability was the original design goal. The production metrics — 2.8B tokens/month, 99.97% availability, 34% cache hit rate, 23% latency reduction, 31% cost reduction — are the measured outcomes of running that design at scale.\"}],\"\\n\",[\"$\",\"$L19\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L1a\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"# Same code, different model, different provider:\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6F42C1\",\"--shiki-dark\":\"#B392F0\"},\"children\":\"curl\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" https://llm.hanzo.ai/v1/chat/completions\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" \\\\\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"  -H\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" \\\"Authorization: Bearer \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"$$HANZO_API_KEY\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" \\\\\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"  -d\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" '{\\\"model\\\": \\\"zen4\\\", \\\"messages\\\": [...]}'\"}]]}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The gateway is available at \",[\"$\",\"$L1b\",null,{\"href\":\"https://llm.hanzo.ai\",\"children\":\"llm.hanzo.ai\"}],\" and the source is open at \",[\"$\",\"$L1b\",null,{\"href\":\"https://github.com/hanzoai/llm\",\"children\":\"github.com/hanzoai/llm\"}],\".\"]}]],\"className\":\"prose\"}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-10\",\"children\":[\"$\",\"section\",null,{\"className\":\"border-t border-border p-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium mb-8\",\"children\":\"Read more\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-8\",\"children\":[[\"$\",\"$L8\",\"/blog/2024-03-12-llm-infrastructure\",{\"href\":\"/blog/2024-03-12-llm-infrastructure\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"LLM Infrastructure: Running AI at Scale\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"How we built infrastructure to serve billions of LLM requests for commerce applications.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"March 11, 2024\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2026-02-27-full-stack-ai-agents\",{\"href\":\"/blog/2026-02-27-full-stack-ai-agents\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"The Complete AI Agent Stack: Models, Compute, and Tools in One Platform\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Hanzo AI introduces the first platform combining 100+ AI models, cloud compute, GPU access, and 260+ MCP tools under a single developer account.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"February 26, 2026\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2026-02-27-unified-ai-gateway\",{\"href\":\"/blog/2026-02-27-unified-ai-gateway\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"One API for Every AI Model: Introducing the Hanzo AI Gateway\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Hanzo AI launches the industry's first zero-markup multi-provider AI gateway — one API key for 100+ models from every major provider, plus 14 proprietary Zen models.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"February 26, 2026\"}]]}]]}]]}]]}]}]}]]}],[\"$\",\"aside\",null,{\"className\":\"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20\",\"children\":[\"$\",\"div\",null,{\"className\":\"sticky top-20 space-y-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-start gap-2\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-sm tracking-tight text-balance font-semibold\",\"children\":\"Hanzo AI\"}]}]}],[\"$\",\"div\",null,{\"className\":\"border border-border rounded-lg p-6 bg-card\",\"children\":[\"$\",\"$L1c\",null,{}]}]]}]}]]}],[\"$\",\"$L1d\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"black\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale - Hanzo Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"author\",\"href\":\"https://hanzo.blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"author\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale,infrastructure,llm,gateway,production,ai,Hanzo AI,Blog,AI Research,Infrastructure\"}],[\"$\",\"meta\",\"5\",{\"name\":\"creator\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"6\",{\"name\":\"publisher\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"7\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"8\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"9\",{\"rel\":\"canonical\",\"href\":\"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:site_name\",\"content\":\"Hanzo Blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image\",\"content\":\"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:image:alt\",\"content\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale\"}],[\"$\",\"meta\",\"18\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:published_time\",\"content\":\"2026-02-10\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:author\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"infrastructure\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"llm\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"gateway\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"production\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"ai\"}],[\"$\",\"meta\",\"26\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"27\",{\"name\":\"twitter:site\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"28\",{\"name\":\"twitter:creator\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"29\",{\"name\":\"twitter:title\",\"content\":\"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:description\",\"content\":\"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production.\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:image\",\"content\":\"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"16:{\"metadata\":\"$e:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>