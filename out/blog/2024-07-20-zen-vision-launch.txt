1:"$Sreact.fragment"
2:I[861,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"PostHogProvider"]
3:I[2710,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"ThemeProvider"]
4:I[9933,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Image"]
5:I[3977,[],""]
6:I[8765,[],""]
7:I[4526,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"FlickeringGrid"]
8:I[1964,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],""]
a:I[2463,[],"OutletBoundary"]
d:I[373,[],"AsyncMetadataOutlet"]
f:I[2463,[],"ViewportBoundary"]
11:I[2463,[],"MetadataBoundary"]
13:I[680,[],""]
:HL["/_next/static/media/27834908180db20f-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/78fec81b34c4a365.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/45441016e214994e.css","style"]
0:{"P":null,"b":"yeRLKKIdrV-2cpcPU6Jw-","p":"","c":["","blog","2024-07-20-zen-vision-launch"],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","2024-07-20-zen-vision-launch","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/45441016e214994e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_245d8d __variable_97c177 antialiased","suppressHydrationWarning":true,"children":["$undefined",["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","header",null,{"className":"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60","children":["$","div",null,{"className":"max-w-5xl mx-auto flex items-center justify-between","children":[["$","a",null,{"href":"https://hanzo.ai","className":"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":20,"height":20,"className":"dark:invert-0 invert"}],["$","span",null,{"className":"font-semibold text-base tracking-tight","children":"hanzo"}],["$","span",null,{"className":"text-muted-foreground text-sm","children":"/ blog"}]]}],["$","nav",null,{"className":"flex items-center gap-4 text-sm text-muted-foreground","children":[["$","a",null,{"href":"https://hanzo.ai","className":"hover:text-foreground transition-colors","children":"hanzo.ai"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay"}],["$","a",null,{"href":"https://discord.gg/hanzo","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-message-circle h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","vv11sd",{"d":"M7.9 20A9 9 0 1 0 4 16.1L2 22Z"}],"$undefined"]}],"Discord"]}]]}]]}]}],["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"min-h-screen bg-background flex items-center justify-center w-full z-10","children":[["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.5,"flickerChance":0.1}]}],["$","div",null,{"className":"text-center flex flex-col gap-4 max-w-xs mx-auto relative","children":[["$","h1",null,{"className":"text-8xl font-mono font-bold drop-shadow-lg text-primary","children":"404"}],["$","p",null,{"className":"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance","children":"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL."}],["$","$L8",null,{"href":"/","children":"Back to Home","className":"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[>svg]:px-3 w-full rounded-lg h-9 drop-shadow-lg","ref":null}]]}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"border-t border-border/50 px-6 py-6 mt-auto","children":["$","div",null,{"className":"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground","children":[["$","div",null,{"className":"flex items-center gap-2","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":16,"height":16,"className":"dark:invert-0 invert opacity-50"}],["$","span",null,{"children":"© 2025 Hanzo AI, Inc. Techstars '17."}]]}],["$","div",null,{"className":"flex items-center gap-4","children":[["$","a",null,{"href":"https://hanzo.ai/privacy","className":"hover:text-foreground transition-colors","children":"Privacy"}],["$","a",null,{"href":"https://hanzo.ai/terms","className":"hover:text-foreground transition-colors","children":"Terms"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo blog"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen blog"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay.blog"}],["$","a",null,{"href":"https://github.com/hanzoai","target":"_blank","rel":"noopener noreferrer","className":"hover:text-foreground transition-colors","children":"GitHub"}]]}]]}]}]]}]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","2024-07-20-zen-vision-launch","d"],["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L9",null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","G_X2tV2wHLh94QPAEbTY9v",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$L11",null,{"children":"$L12"}]]}],false]],"m":"$undefined","G":["$13","$undefined"],"s":false,"S":true}
14:"$Sreact.suspense"
15:I[373,[],"AsyncMetadata"]
17:I[9048,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"HashScrollHandler"]
18:I[4170,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CopyHeader"]
19:I[4255,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CodeBlock"]
1b:I[4255,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Pre"]
1c:I[3979,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"default"]
1d:I[9372,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"TableOfContents"]
1e:I[1413,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"MobileTableOfContents"]
12:["$","div",null,{"hidden":true,"children":["$","$14",null,{"fallback":null,"children":["$","$L15",null,{"promise":"$@16"}]}]}]
1a:T5c0,<svg viewBox="0 0 24 24"><path d="M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z" fill="currentColor" /></svg>9:["$","div",null,{"className":"min-h-screen bg-background relative","children":[["$","$L17",null,{}],["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.2,"flickerChance":0.05}]}],["$","div",null,{"className":"space-y-4 border-b border-border relative z-10","children":["$","div",null,{"className":"max-w-7xl mx-auto flex flex-col gap-6 p-6","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground","children":[["$","$L8",null,{"href":"/","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left w-4 h-4","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"Back to all articles"}]],"className":"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 h-6 w-6","ref":null}],["$","div",null,{"className":"flex flex-wrap gap-3 text-muted-foreground","children":[["$","span","ai",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"ai"}],["$","span","models",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"models"}],["$","span","zen",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen"}],["$","span","vision",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"vision"}],["$","span","multimodal",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"multimodal"}],["$","span","launch",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"launch"}],["$","span","zen-mode",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen-mode"}]]}],["$","time",null,{"className":"font-medium text-muted-foreground","children":"July 19, 2024"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance","children":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","p",null,{"className":"text-muted-foreground max-w-4xl md:text-lg md:text-balance","children":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}]]}]}],["$","div",null,{"className":"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10","children":[["$","div",null,{"className":"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"}],["$","main",null,{"className":"w-full p-0 overflow-hidden","children":[null,["$","div",null,{"className":"p-6 lg:p-10","children":["$","div",null,{"className":"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg","children":["$","div",null,{"ref":"$undefined","children":[["$","$L18",null,{"level":1,"id":"zen-vision-multimodal-understanding-at-72b-scale","children":"Zen Vision: Multimodal Understanding at 72B Scale"}],"\n",["$","p",null,{"children":"Text is only part of what people work with. Documents have tables, charts, and diagrams. UIs are screenshots. Engineering artifacts are architecture diagrams. Medical records contain scanned images. Code lives inside images on Stack Overflow. A language model that cannot see is half a model."}],"\n",["$","p",null,{"children":"Zen Vision closes that gap. Today we are releasing a 72B multimodal model that understands images and text together, trained from the ground up to reason across both modalities rather than treating vision as a bolt-on patch to a text model."}],"\n",["$","$L18",null,{"level":2,"id":"architecture","children":"Architecture"}],"\n",["$","p",null,{"children":"Zen Vision uses a native multimodal architecture: a 72B language model backbone with a deep vision encoder coupled at every transformer layer, not just at input embedding time. This architecture decision matters."}],"\n",["$","p",null,{"children":"Most \"multimodal\" models process the image once with a vision encoder, convert it to a flat embedding, and hand it to the language model as if it were just more tokens. This works for simple captioning tasks. It fails when the task requires spatial reasoning — understanding that a chart has a y-axis label on the left, a legend at the top right, and a data series that peaks in Q3. Shallow coupling loses that spatial structure."}],"\n",["$","p",null,{"children":"In Zen Vision, the vision encoder and language backbone run in coupled attention at multiple depths. The language model can query the visual features directly, multiple times, at different levels of abstraction. High-level features (\"this is a bar chart\") and low-level features (\"the bar at position 3 reaches pixel height 287\") are available throughout reasoning."}],"\n",["$","$L18",null,{"level":3,"id":"vision-encoder","children":"Vision Encoder"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Parameters"}],": 72B backbone + 2B vision encoder"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Input resolution"}],": Up to 4096×4096 pixels, tiled automatically"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Tile size"}],": 448×448 with 50% overlap for boundary continuity"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Max images per request"}],": 16 images at full resolution, or mixed-resolution batches"]}],"\n"]}],"\n",["$","$L18",null,{"level":2,"id":"capabilities","children":"Capabilities"}],"\n",["$","$L18",null,{"level":3,"id":"ocr-and-document-extraction","children":"OCR and Document Extraction"}],"\n",["$","p",null,{"children":"Zen Vision handles handwritten text, printed text, mixed scripts, and degraded scans. On the DocVQA benchmark (document visual question answering), Zen Vision scores 91.4 — above the previous Zen architecture and competitive with the best specialized OCR models."}],"\n",["$","p",null,{"children":"It extracts structured data from documents: pull a table from a scanned invoice as JSON, extract line items from a purchase order, parse a handwritten form into key-value pairs. This is not template matching — it generalizes to novel document layouts."}],"\n",["$","$L18",null,{"level":3,"id":"diagram-reasoning","children":"Diagram Reasoning"}],"\n",["$","p",null,{"children":"Architecture diagrams, flowcharts, network topologies, ER diagrams, circuit schematics. Zen Vision reads them, describes them, and answers questions about them. Given a system architecture diagram, it can identify single points of failure. Given a flowchart, it can trace execution paths. Given a circuit schematic, it can identify the signal flow."}],"\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Task"}],["$","th",null,{"children":"Zen Vision"}],["$","th",null,{"children":"Baseline"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"DocVQA"}],["$","td",null,{"children":"91.4"}],["$","td",null,{"children":"84.2"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"ChartQA"}],["$","td",null,{"children":"88.7"}],["$","td",null,{"children":"82.1"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"TextVQA"}],["$","td",null,{"children":"82.3"}],["$","td",null,{"children":"76.8"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"MMBench"}],["$","td",null,{"children":"84.1"}],["$","td",null,{"children":"79.3"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"OCRBench"}],["$","td",null,{"children":"79.8"}],["$","td",null,{"children":"71.4"}]]}]]}]]}]}],"\n",["$","$L18",null,{"level":3,"id":"screenshot-analysis","children":"Screenshot Analysis"}],"\n",["$","p",null,{"children":"UI screenshots are a first-class use case. Zen Vision can describe what is on screen, identify UI components, extract text from rendered interfaces, answer questions about layout, and explain what a user would need to do to accomplish a task. This makes it directly useful for:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Automated UI testing with natural language assertions"}],"\n",["$","li",null,{"children":"Accessibility auditing of screenshots"}],"\n",["$","li",null,{"children":"Visual regression descriptions"}],"\n",["$","li",null,{"children":"Documentation generation from screenshots"}],"\n"]}],"\n",["$","$L18",null,{"level":3,"id":"code-in-images","children":"Code in Images"}],"\n",["$","p",null,{"children":"Stack Overflow, documentation, tutorial screenshots — code lives inside images constantly. Zen Vision reads code from images accurately, including code with unusual formatting, syntax highlighting, or partial visibility."}],"\n",["$","$L18",null,{"level":2,"id":"limitations","children":"Limitations"}],"\n",["$","p",null,{"children":"Zen Vision is a reasoning model, not a pixel counter. It does not replace specialized OCR pipelines for extremely high-volume structured extraction where speed and cost dominate. At 72B, it is also not a mobile model — inference requires significant GPU memory."}],"\n",["$","p",null,{"children":"The 16-image limit per request is a practical constraint of current serving infrastructure, not an architectural limit. We are working on increasing this."}],"\n",["$","$L18",null,{"level":2,"id":"usage","children":"Usage"}],"\n",["$","$L19",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"$1a","children":["$","$L1b",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"import"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" hanzo"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"client "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" hanzo.Client("}],["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"api_key"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"...\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"response "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" client.chat.completions.create("}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    model"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"zen-vision\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    messages"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"[{"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"role\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"user\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"content\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": ["}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"            {"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"type\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"image_url\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":", "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"image_url\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": {"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"url\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"data:image/jpeg;base64,...\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"}},"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"            {"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"type\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"text\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":", "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"text\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"Extract all line items from this invoice as JSON.\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"}"}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"        ]"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    }]"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]}]]}]}]}],"\n",["$","p",null,{"children":["Available via Hanzo Cloud at ",["$","code",null,{"children":"api.hanzo.ai/v1/chat/completions"}]," with model ",["$","code",null,{"children":"zen-vision"}],". Weights available at ",["$","$L1c",null,{"href":"https://huggingface.co/zenlm/zen-vision","children":"huggingface.co/zenlm/zen-vision"}],"."]}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Zach Kelling is the founder of Hanzo AI, Techstars '17."}]}]],"className":"prose"}]}]}],["$","div",null,{"className":"mt-10","children":["$","section",null,{"className":"border-t border-border p-0","children":["$","div",null,{"className":"p-6 lg:p-10","children":[["$","h2",null,{"className":"text-2xl font-medium mb-8","children":"Read more"}],["$","div",null,{"className":"flex flex-col gap-8","children":[["$","$L8","/blog/2025-07-01-zen-designer-launch",{"href":"/blog/2025-07-01-zen-designer-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Designer: 235B Vision-Language Model"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"June 30, 2025"}]]}]]}],["$","$L8","/blog/2025-08-01-zen-omni-launch",{"href":"/blog/2025-08-01-zen-omni-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Omni: Unified Multimodal AI"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"July 31, 2025"}]]}]]}],["$","$L8","/blog/2025-10-10-zen-audit-launch",{"href":"/blog/2025-10-10-zen-audit-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Audit: Code Security and Smart Contract Analysis"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Audit is trained on CVE databases, audit reports, and vulnerability research to provide automated code security analysis and smart contract auditing with low false positive rates."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"October 9, 2025"}]]}]]}]]}]]}]}]}]]}],["$","aside",null,{"className":"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20","children":["$","div",null,{"className":"sticky top-20 space-y-8","children":[["$","div",null,{"className":"flex items-start gap-2","children":["$","div",null,{"className":"flex-1","children":["$","h3",null,{"className":"text-sm tracking-tight text-balance font-semibold","children":"Zach Kelling"}]}]}],["$","div",null,{"className":"border border-border rounded-lg p-6 bg-card","children":["$","$L1d",null,{}]}]]}]}]]}],["$","$L1e",null,{}]]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","content":"black"}]]
b:null
e:{"metadata":[["$","title","0",{"children":"Zen Vision: Multimodal Understanding at 72B Scale - Hanzo Blog"}],["$","meta","1",{"name":"description","content":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}],["$","link","2",{"rel":"author","href":"https://hanzo.blog"}],["$","meta","3",{"name":"author","content":"Zach Kelling"}],["$","meta","4",{"name":"keywords","content":"Zen Vision: Multimodal Understanding at 72B Scale,ai,models,zen,vision,multimodal,launch,zen-mode,Hanzo AI,Blog,AI Research,Infrastructure"}],["$","meta","5",{"name":"creator","content":"Zach Kelling"}],["$","meta","6",{"name":"publisher","content":"Hanzo AI"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://hanzo.blog/blog/2024-07-20-zen-vision-launch"}],["$","meta","10",{"property":"og:title","content":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","meta","11",{"property":"og:description","content":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}],["$","meta","12",{"property":"og:url","content":"https://hanzo.blog/blog/2024-07-20-zen-vision-launch"}],["$","meta","13",{"property":"og:site_name","content":"Hanzo Blog"}],["$","meta","14",{"property":"og:image","content":"https://hanzo.blog/blog/2024-07-20-zen-vision-launch/opengraph-image"}],["$","meta","15",{"property":"og:image:width","content":"1200"}],["$","meta","16",{"property":"og:image:height","content":"630"}],["$","meta","17",{"property":"og:image:alt","content":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","meta","18",{"property":"og:type","content":"article"}],["$","meta","19",{"property":"article:published_time","content":"2024-07-20"}],["$","meta","20",{"property":"article:author","content":"Zach Kelling"}],["$","meta","21",{"property":"article:tag","content":"ai"}],["$","meta","22",{"property":"article:tag","content":"models"}],["$","meta","23",{"property":"article:tag","content":"zen"}],["$","meta","24",{"property":"article:tag","content":"vision"}],["$","meta","25",{"property":"article:tag","content":"multimodal"}],["$","meta","26",{"property":"article:tag","content":"launch"}],["$","meta","27",{"property":"article:tag","content":"zen-mode"}],["$","meta","28",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","29",{"name":"twitter:site","content":"@hanzoai"}],["$","meta","30",{"name":"twitter:creator","content":"@hanzoai"}],["$","meta","31",{"name":"twitter:title","content":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","meta","32",{"name":"twitter:description","content":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}],["$","meta","33",{"name":"twitter:image","content":"https://hanzo.blog/blog/2024-07-20-zen-vision-launch/opengraph-image"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$e:metadata","error":null,"digest":"$undefined"}
