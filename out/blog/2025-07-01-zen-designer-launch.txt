1:"$Sreact.fragment"
2:I[861,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"PostHogProvider"]
3:I[2710,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"ThemeProvider"]
4:I[9933,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Image"]
5:I[3977,[],""]
6:I[8765,[],""]
7:I[4526,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"FlickeringGrid"]
8:I[1964,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],""]
a:I[2463,[],"OutletBoundary"]
d:I[373,[],"AsyncMetadataOutlet"]
f:I[2463,[],"ViewportBoundary"]
11:I[2463,[],"MetadataBoundary"]
13:I[680,[],""]
:HL["/_next/static/media/27834908180db20f-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/78fec81b34c4a365.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/45441016e214994e.css","style"]
0:{"P":null,"b":"yeRLKKIdrV-2cpcPU6Jw-","p":"","c":["","blog","2025-07-01-zen-designer-launch"],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","2025-07-01-zen-designer-launch","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/45441016e214994e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_245d8d __variable_97c177 antialiased","suppressHydrationWarning":true,"children":["$undefined",["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","header",null,{"className":"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60","children":["$","div",null,{"className":"max-w-5xl mx-auto flex items-center justify-between","children":[["$","a",null,{"href":"https://hanzo.ai","className":"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":20,"height":20,"className":"dark:invert-0 invert"}],["$","span",null,{"className":"font-semibold text-base tracking-tight","children":"hanzo"}],["$","span",null,{"className":"text-muted-foreground text-sm","children":"/ blog"}]]}],["$","nav",null,{"className":"flex items-center gap-4 text-sm text-muted-foreground","children":[["$","a",null,{"href":"https://hanzo.ai","className":"hover:text-foreground transition-colors","children":"hanzo.ai"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay"}],["$","a",null,{"href":"https://discord.gg/hanzo","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-message-circle h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","vv11sd",{"d":"M7.9 20A9 9 0 1 0 4 16.1L2 22Z"}],"$undefined"]}],"Discord"]}]]}]]}]}],["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"min-h-screen bg-background flex items-center justify-center w-full z-10","children":[["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.5,"flickerChance":0.1}]}],["$","div",null,{"className":"text-center flex flex-col gap-4 max-w-xs mx-auto relative","children":[["$","h1",null,{"className":"text-8xl font-mono font-bold drop-shadow-lg text-primary","children":"404"}],["$","p",null,{"className":"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance","children":"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL."}],["$","$L8",null,{"href":"/","children":"Back to Home","className":"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[>svg]:px-3 w-full rounded-lg h-9 drop-shadow-lg","ref":null}]]}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"border-t border-border/50 px-6 py-6 mt-auto","children":["$","div",null,{"className":"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground","children":[["$","div",null,{"className":"flex items-center gap-2","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":16,"height":16,"className":"dark:invert-0 invert opacity-50"}],["$","span",null,{"children":"Â© 2025 Hanzo AI, Inc. Techstars '17."}]]}],["$","div",null,{"className":"flex items-center gap-4","children":[["$","a",null,{"href":"https://hanzo.ai/privacy","className":"hover:text-foreground transition-colors","children":"Privacy"}],["$","a",null,{"href":"https://hanzo.ai/terms","className":"hover:text-foreground transition-colors","children":"Terms"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo blog"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen blog"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay.blog"}],["$","a",null,{"href":"https://github.com/hanzoai","target":"_blank","rel":"noopener noreferrer","className":"hover:text-foreground transition-colors","children":"GitHub"}]]}]]}]}]]}]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","2025-07-01-zen-designer-launch","d"],["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L9",null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","EQXcsGawoR0jVRCEEjCJWv",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$L11",null,{"children":"$L12"}]]}],false]],"m":"$undefined","G":["$13","$undefined"],"s":false,"S":true}
14:"$Sreact.suspense"
15:I[373,[],"AsyncMetadata"]
17:I[9048,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"HashScrollHandler"]
18:I[4170,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CopyHeader"]
19:I[3979,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"default"]
1a:I[9372,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"TableOfContents"]
1b:I[1413,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"MobileTableOfContents"]
12:["$","div",null,{"hidden":true,"children":["$","$14",null,{"fallback":null,"children":["$","$L15",null,{"promise":"$@16"}]}]}]
9:["$","div",null,{"className":"min-h-screen bg-background relative","children":[["$","$L17",null,{}],["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.2,"flickerChance":0.05}]}],["$","div",null,{"className":"space-y-4 border-b border-border relative z-10","children":["$","div",null,{"className":"max-w-7xl mx-auto flex flex-col gap-6 p-6","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground","children":[["$","$L8",null,{"href":"/","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left w-4 h-4","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"Back to all articles"}]],"className":"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 h-6 w-6","ref":null}],["$","div",null,{"className":"flex flex-wrap gap-3 text-muted-foreground","children":[["$","span","ai",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"ai"}],["$","span","models",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"models"}],["$","span","zen",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen"}],["$","span","vision",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"vision"}],["$","span","multimodal",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"multimodal"}],["$","span","launch",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"launch"}],["$","span","design",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"design"}],["$","span","zen-mode",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"zen-mode"}]]}],["$","time",null,{"className":"font-medium text-muted-foreground","children":"June 30, 2025"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance","children":"Zen Designer: 235B Vision-Language Model"}],["$","p",null,{"className":"text-muted-foreground max-w-4xl md:text-lg md:text-balance","children":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}]]}]}],["$","div",null,{"className":"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10","children":[["$","div",null,{"className":"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"}],["$","main",null,{"className":"w-full p-0 overflow-hidden","children":[null,["$","div",null,{"className":"p-6 lg:p-10","children":["$","div",null,{"className":"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg","children":["$","div",null,{"ref":"$undefined","children":[["$","$L18",null,{"level":1,"id":"zen-designer-235b-vision-language-model","children":"Zen Designer: 235B Vision-Language Model"}],"\n",["$","p",null,{"children":"Zen Designer is a 235B mixture-of-experts vision-language model built for visual understanding and design reasoning. 22B parameters active per forward pass, with native support for images, video, OCR across 32 languages, and layout analysis."}],"\n",["$","$L18",null,{"level":2,"id":"architecture","children":"Architecture"}],"\n",["$","p",null,{"children":"235B total parameters across a sparse MoE architecture, with 22B activated per token. The vision encoder processes images up to 4K resolution with no degradation on dense visual inputs -- detailed charts, fine text, complex diagrams. The language model handles visual context natively rather than late-fusing separate embeddings."}],"\n",["$","$L18",null,{"level":2,"id":"benchmark-results","children":"Benchmark Results"}],"\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Benchmark"}],["$","th",null,{"children":"Score"}],["$","th",null,{"children":"Task Type"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"DocVQA"}],["$","td",null,{"children":["$","strong",null,{"children":"94.1"}]}],["$","td",null,{"children":"Document question answering"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"ChartQA"}],["$","td",null,{"children":["$","strong",null,{"children":"88.3"}]}],["$","td",null,{"children":"Chart and graph understanding"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"MMBench"}],["$","td",null,{"children":["$","strong",null,{"children":"87.6"}]}],["$","td",null,{"children":"Multimodal reasoning"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"TextVQA"}],["$","td",null,{"children":["$","strong",null,{"children":"85.2"}]}],["$","td",null,{"children":"Text recognition in natural images"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"MathVista"}],["$","td",null,{"children":["$","strong",null,{"children":"74.8"}]}],["$","td",null,{"children":"Visual math problem solving"}]]}]]}]]}]}],"\n",["$","p",null,{"children":"DocVQA at 94.1 measures the ability to answer questions from scanned documents, invoices, forms, and PDFs. This requires combining OCR, spatial understanding, and reasoning -- not just recognizing text but understanding what it means in context."}],"\n",["$","$L18",null,{"level":2,"id":"ocr-in-32-languages","children":"OCR in 32 Languages"}],"\n",["$","p",null,{"children":"Text recognition in 32 languages with full Unicode handling:"}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Western"}],": English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Polish, Czech, Swedish, Norwegian, Finnish, Danish, Romanian"]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"East Asian"}],": Chinese Simplified, Chinese Traditional, Japanese, Korean"]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Southeast Asian"}],": Thai, Vietnamese, Indonesian"]}],"\n",["$","p",null,{"children":[["$","strong",null,{"children":"Other"}],": Arabic, Hebrew, Hindi, Bengali, Turkish, Greek, Hungarian, Croatian, Catalan, Ukrainian"]}],"\n",["$","p",null,{"children":"OCR quality is consistent across languages -- not an English-first model with degraded support elsewhere. Training explicitly included high-density multilingual document corpora."}],"\n",["$","$L18",null,{"level":2,"id":"capabilities","children":"Capabilities"}],"\n",["$","$L18",null,{"level":3,"id":"image-analysis","children":"Image Analysis"}],"\n",["$","p",null,{"children":"Natural-language description of any image at multiple levels of detail. Zen Designer understands objects, spatial relationships, text, color, composition, and intent. Ask \"what is wrong with this UI\" and it will identify specific layout problems, contrast issues, and accessibility concerns."}],"\n",["$","$L18",null,{"level":3,"id":"video-understanding","children":"Video Understanding"}],"\n",["$","p",null,{"children":"Frame-by-frame temporal analysis. Zen Designer processes video as a sequence of visual tokens, maintaining temporal context across frames. Use cases include product demo analysis, design review recordings, and visual QA testing."}],"\n",["$","$L18",null,{"level":3,"id":"text-to-layout","children":"Text-to-Layout"}],"\n",["$","p",null,{"children":"Describe a layout in natural language and Zen Designer generates structured layout specifications -- bounding boxes, component hierarchies, spacing rules. Useful for bridging design intent to implementation."}],"\n",["$","$L18",null,{"level":3,"id":"visual-reasoning","children":"Visual Reasoning"}],"\n",["$","p",null,{"children":"The model reasons about what it sees, not just describes it. Questions like \"which chart shows the highest growth rate after Q3\" or \"which of these wireframes better follows accessibility guidelines\" get reasoned answers, not just observations."}],"\n",["$","$L18",null,{"level":3,"id":"design-analysis","children":"Design Analysis"}],"\n",["$","p",null,{"children":"Zen Designer understands design systems: component relationships, visual hierarchy, typography, grid alignment, color theory. It can audit a screenshot against a design specification or identify deviations from a style guide."}],"\n",["$","$L18",null,{"level":2,"id":"get-zen-designer","children":"Get Zen Designer"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"HuggingFace"}],": ",["$","$L19",null,{"href":"https://huggingface.co/zenlm","children":"huggingface.co/zenlm"}]]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Hanzo Cloud API"}],": ",["$","code",null,{"children":"api.hanzo.ai/v1/chat/completions"}]," -- model ",["$","code",null,{"children":"zen-designer"}]]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Zen LM"}],": ",["$","$L19",null,{"href":"https://zenlm.org","children":"zenlm.org"}]," -- vision API guides and multimodal examples"]}],"\n"]}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":["$","em",null,{"children":"Zach Kelling is the founder of Hanzo AI, Techstars '17."}]}]],"className":"prose"}]}]}],["$","div",null,{"className":"mt-10","children":["$","section",null,{"className":"border-t border-border p-0","children":["$","div",null,{"className":"p-6 lg:p-10","children":[["$","h2",null,{"className":"text-2xl font-medium mb-8","children":"Read more"}],["$","div",null,{"className":"flex flex-col gap-8","children":[["$","$L8","/blog/2024-07-20-zen-vision-launch",{"href":"/blog/2024-07-20-zen-vision-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Vision: Multimodal Understanding at 72B Scale"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"July 19, 2024"}]]}]]}],["$","$L8","/blog/2025-08-01-zen-omni-launch",{"href":"/blog/2025-08-01-zen-omni-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Omni: Unified Multimodal AI"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"July 31, 2025"}]]}]]}],["$","$L8","/blog/2025-10-10-zen-audit-launch",{"href":"/blog/2025-10-10-zen-audit-launch","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"Zen Audit: Code Security and Smart Contract Analysis"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Zen Audit is trained on CVE databases, audit reports, and vulnerability research to provide automated code security analysis and smart contract auditing with low false positive rates."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"October 9, 2025"}]]}]]}]]}]]}]}]}]]}],["$","aside",null,{"className":"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20","children":["$","div",null,{"className":"sticky top-20 space-y-8","children":[["$","div",null,{"className":"flex items-start gap-2","children":["$","div",null,{"className":"flex-1","children":["$","h3",null,{"className":"text-sm tracking-tight text-balance font-semibold","children":"Zach Kelling"}]}]}],["$","div",null,{"className":"border border-border rounded-lg p-6 bg-card","children":["$","$L1a",null,{}]}]]}]}]]}],["$","$L1b",null,{}]]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","content":"black"}]]
b:null
e:{"metadata":[["$","title","0",{"children":"Zen Designer: 235B Vision-Language Model - Hanzo Blog"}],["$","meta","1",{"name":"description","content":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}],["$","link","2",{"rel":"author","href":"https://hanzo.blog"}],["$","meta","3",{"name":"author","content":"Zach Kelling"}],["$","meta","4",{"name":"keywords","content":"Zen Designer: 235B Vision-Language Model,ai,models,zen,vision,multimodal,launch,design,zen-mode,Hanzo AI,Blog,AI Research,Infrastructure"}],["$","meta","5",{"name":"creator","content":"Zach Kelling"}],["$","meta","6",{"name":"publisher","content":"Hanzo AI"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://hanzo.blog/blog/2025-07-01-zen-designer-launch"}],["$","meta","10",{"property":"og:title","content":"Zen Designer: 235B Vision-Language Model"}],["$","meta","11",{"property":"og:description","content":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}],["$","meta","12",{"property":"og:url","content":"https://hanzo.blog/blog/2025-07-01-zen-designer-launch"}],["$","meta","13",{"property":"og:site_name","content":"Hanzo Blog"}],["$","meta","14",{"property":"og:image","content":"https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image"}],["$","meta","15",{"property":"og:image:width","content":"1200"}],["$","meta","16",{"property":"og:image:height","content":"630"}],["$","meta","17",{"property":"og:image:alt","content":"Zen Designer: 235B Vision-Language Model"}],["$","meta","18",{"property":"og:type","content":"article"}],["$","meta","19",{"property":"article:published_time","content":"2025-07-01"}],["$","meta","20",{"property":"article:author","content":"Zach Kelling"}],["$","meta","21",{"property":"article:tag","content":"ai"}],["$","meta","22",{"property":"article:tag","content":"models"}],["$","meta","23",{"property":"article:tag","content":"zen"}],["$","meta","24",{"property":"article:tag","content":"vision"}],["$","meta","25",{"property":"article:tag","content":"multimodal"}],["$","meta","26",{"property":"article:tag","content":"launch"}],["$","meta","27",{"property":"article:tag","content":"design"}],["$","meta","28",{"property":"article:tag","content":"zen-mode"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:site","content":"@hanzoai"}],["$","meta","31",{"name":"twitter:creator","content":"@hanzoai"}],["$","meta","32",{"name":"twitter:title","content":"Zen Designer: 235B Vision-Language Model"}],["$","meta","33",{"name":"twitter:description","content":"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."}],["$","meta","34",{"name":"twitter:image","content":"https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$e:metadata","error":null,"digest":"$undefined"}
