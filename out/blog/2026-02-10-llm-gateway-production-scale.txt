1:"$Sreact.fragment"
2:I[861,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"PostHogProvider"]
3:I[2710,["803","static/chunks/cd24890f-00c67ea9d0bbf445.js","933","static/chunks/933-166e0fcc9496c98f.js","177","static/chunks/app/layout-d687a3a56998d23e.js"],"ThemeProvider"]
4:I[9933,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Image"]
5:I[3977,[],""]
6:I[8765,[],""]
7:I[4526,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"FlickeringGrid"]
8:I[1964,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],""]
a:I[2463,[],"OutletBoundary"]
d:I[373,[],"AsyncMetadataOutlet"]
f:I[2463,[],"ViewportBoundary"]
11:I[2463,[],"MetadataBoundary"]
13:I[680,[],""]
:HL["/_next/static/media/27834908180db20f-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/78fec81b34c4a365.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/45441016e214994e.css","style"]
0:{"P":null,"b":"yeRLKKIdrV-2cpcPU6Jw-","p":"","c":["","blog","2026-02-10-llm-gateway-production-scale"],"i":false,"f":[[["",{"children":["blog",{"children":[["slug","2026-02-10-llm-gateway-production-scale","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/45441016e214994e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_245d8d __variable_97c177 antialiased","suppressHydrationWarning":true,"children":["$undefined",["$","body",null,{"children":["$","$L2",null,{"children":["$","$L3",null,{"attribute":"class","defaultTheme":"dark","enableSystem":true,"disableTransitionOnChange":true,"children":[["$","header",null,{"className":"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60","children":["$","div",null,{"className":"max-w-5xl mx-auto flex items-center justify-between","children":[["$","a",null,{"href":"https://hanzo.ai","className":"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":20,"height":20,"className":"dark:invert-0 invert"}],["$","span",null,{"className":"font-semibold text-base tracking-tight","children":"hanzo"}],["$","span",null,{"className":"text-muted-foreground text-sm","children":"/ blog"}]]}],["$","nav",null,{"className":"flex items-center gap-4 text-sm text-muted-foreground","children":[["$","a",null,{"href":"https://hanzo.ai","className":"hover:text-foreground transition-colors","children":"hanzo.ai"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay"}],["$","a",null,{"href":"https://discord.gg/hanzo","target":"_blank","rel":"noopener noreferrer","className":"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-message-circle h-3.5 w-3.5","aria-hidden":"true","children":[["$","path","vv11sd",{"d":"M7.9 20A9 9 0 1 0 4 16.1L2 22Z"}],"$undefined"]}],"Discord"]}]]}]]}]}],["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"min-h-screen bg-background flex items-center justify-center w-full z-10","children":[["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.5,"flickerChance":0.1}]}],["$","div",null,{"className":"text-center flex flex-col gap-4 max-w-xs mx-auto relative","children":[["$","h1",null,{"className":"text-8xl font-mono font-bold drop-shadow-lg text-primary","children":"404"}],["$","p",null,{"className":"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance","children":"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL."}],["$","$L8",null,{"href":"/","children":"Back to Home","className":"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[>svg]:px-3 w-full rounded-lg h-9 drop-shadow-lg","ref":null}]]}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}],["$","footer",null,{"className":"border-t border-border/50 px-6 py-6 mt-auto","children":["$","div",null,{"className":"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground","children":[["$","div",null,{"className":"flex items-center gap-2","children":[["$","$L4",null,{"src":"/hanzo-logo.svg","alt":"Hanzo","width":16,"height":16,"className":"dark:invert-0 invert opacity-50"}],["$","span",null,{"children":"© 2025 Hanzo AI, Inc. Techstars '17."}]]}],["$","div",null,{"className":"flex items-center gap-4","children":[["$","a",null,{"href":"https://hanzo.ai/privacy","className":"hover:text-foreground transition-colors","children":"Privacy"}],["$","a",null,{"href":"https://hanzo.ai/terms","className":"hover:text-foreground transition-colors","children":"Terms"}],["$","a",null,{"href":"https://blog.zoo.ngo","className":"hover:text-foreground transition-colors hidden sm:block","children":"zoo blog"}],["$","a",null,{"href":"https://zenlm.org/blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zen blog"}],["$","a",null,{"href":"https://zeekay.blog","className":"hover:text-foreground transition-colors hidden sm:block","children":"zeekay.blog"}],["$","a",null,{"href":"https://github.com/hanzoai","target":"_blank","rel":"noopener noreferrer","className":"hover:text-foreground transition-colors","children":"GitHub"}]]}]]}]}]]}]}]}]]}]]}],{"children":["blog",["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","2026-02-10-llm-gateway-production-scale","d"],["$","$1","c",{"children":[null,["$","$L5",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L9",null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","JRFEaAmQYAuiy140IxNYJv",{"children":[["$","$Lf",null,{"children":"$L10"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],["$","$L11",null,{"children":"$L12"}]]}],false]],"m":"$undefined","G":["$13","$undefined"],"s":false,"S":true}
14:"$Sreact.suspense"
15:I[373,[],"AsyncMetadata"]
17:I[9048,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"HashScrollHandler"]
18:I[4170,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CopyHeader"]
19:I[4255,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"CodeBlock"]
1a:I[4255,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"Pre"]
1b:I[3979,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"default"]
1c:I[9372,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"TableOfContents"]
1d:I[1413,["771","static/chunks/771-2096a43097a1ac56.js","933","static/chunks/933-166e0fcc9496c98f.js","698","static/chunks/698-980099c394b58d6f.js","218","static/chunks/218-3bb2d990159730f8.js","953","static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js"],"MobileTableOfContents"]
12:["$","div",null,{"hidden":true,"children":["$","$14",null,{"fallback":null,"children":["$","$L15",null,{"promise":"$@16"}]}]}]
9:["$","div",null,{"className":"min-h-screen bg-background relative","children":[["$","$L17",null,{}],["$","div",null,{"className":"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]","children":["$","$L7",null,{"className":"absolute top-0 left-0 size-full","squareSize":4,"gridGap":6,"color":"#6B7280","maxOpacity":0.2,"flickerChance":0.05}]}],["$","div",null,{"className":"space-y-4 border-b border-border relative z-10","children":["$","div",null,{"className":"max-w-7xl mx-auto flex flex-col gap-6 p-6","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground","children":[["$","$L8",null,{"href":"/","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left w-4 h-4","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],["$","span",null,{"className":"sr-only","children":"Back to all articles"}]],"className":"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[>svg]:px-3 h-6 w-6","ref":null}],["$","div",null,{"className":"flex flex-wrap gap-3 text-muted-foreground","children":[["$","span","infrastructure",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"infrastructure"}],["$","span","llm",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"llm"}],["$","span","gateway",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"gateway"}],["$","span","production",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"production"}],["$","span","ai",{"className":"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center","children":"ai"}]]}],["$","time",null,{"className":"font-medium text-muted-foreground","children":"February 9, 2026"}]]}],["$","h1",null,{"className":"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance","children":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"}],["$","p",null,{"className":"text-muted-foreground max-w-4xl md:text-lg md:text-balance","children":"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."}]]}]}],["$","div",null,{"className":"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10","children":[["$","div",null,{"className":"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"}],["$","main",null,{"className":"w-full p-0 overflow-hidden","children":[null,["$","div",null,{"className":"p-6 lg:p-10","children":["$","div",null,{"className":"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg","children":["$","div",null,{"ref":"$undefined","children":[["$","$L18",null,{"level":1,"id":"28-billion-tokens-per-month-llm-gateway-at-production-scale","children":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"}],"\n",["$","p",null,{"children":"Most teams reach for a single AI provider. They hardcode an API key, ship to production, and discover the failure modes incrementally: provider outages, rate limits, latency spikes, cost surprises, and the organizational friction of switching providers when a better model ships."}],"\n",["$","p",null,{"children":"The Hanzo LLM Gateway was built to eliminate those failure modes. Today it processes 2.8 billion tokens per month across 100+ providers at 99.97% availability. Here is how it actually works."}],"\n",["$","$L18",null,{"level":2,"id":"what-9997-means","children":"What 99.97% Means"}],"\n",["$","p",null,{"children":"99.97% availability is approximately 2.6 hours of downtime per year. For a system routing to 100+ third-party providers — each with their own availability characteristics, rate limits, and deployment schedules — this number does not happen by accident."}],"\n",["$","p",null,{"children":"The gateway achieves it through layered redundancy. No single provider failure causes gateway unavailability. Failover chains are configured per-model-class: if provider A returns a 503, the request routes to provider B with the semantically closest model at comparable price. If provider B also fails, it routes to provider C. The fallback chain is evaluated in milliseconds; the user sees elevated latency, not an error."}],"\n",["$","p",null,{"children":"Streaming responses require special handling in failover. A streaming request that begins successfully from provider A cannot be transparently rerouted mid-stream if provider A drops the connection. The gateway's streaming-aware load balancer tracks active stream health and executes a clean restart with the retry provider when a stream fails before the first token is delivered. Mid-stream failures surface as errors — this is honest rather than attempting a seamless splice that would corrupt output."}],"\n",["$","$L18",null,{"level":2,"id":"semantic-caching","children":"Semantic Caching"}],"\n",["$","p",null,{"children":"The 34% cache hit rate is the most operationally significant number after availability. A cache hit means the response is served in under 2 milliseconds without touching any AI provider. For the approximately one-third of requests that hit cache, the effective cost is near zero."}],"\n",["$","p",null,{"children":"Semantic caching differs from exact-match caching. Two prompts that ask the same question with different phrasing — \"translate this to French\" versus \"convert this text to French\" — are semantically equivalent. An exact-match cache misses this; a semantic cache catches it."}],"\n",["$","p",null,{"children":"The implementation uses embedding similarity over a vector index of cached prompt-response pairs. An incoming prompt is embedded; if the nearest cached embedding is within the similarity threshold, the cached response is returned. The threshold is configurable per model class — lower threshold for factual Q&A (where small prompt changes may produce importantly different answers), higher threshold for classification tasks (where semantically similar prompts reliably have the same answer)."}],"\n",["$","p",null,{"children":"Cache invalidation is TTL-based per model. Models that update frequently (rolling deployments of fine-tuned models) use short TTLs. Stable models use longer TTLs. The 34% hit rate is the aggregate across all model classes and TTL configurations."}],"\n",["$","$L18",null,{"level":2,"id":"streaming-aware-load-balancing","children":"Streaming-Aware Load Balancing"}],"\n",["$","p",null,{"children":"The load balancer tracks provider health per model endpoint, not per provider globally. A provider can be degraded for a specific model while healthy for others. Health signals are:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Response latency (p50, p95, p99 over a rolling window)"}],"\n",["$","li",null,{"children":"Error rate (4xx and 5xx, weighted by recency)"}],"\n",["$","li",null,{"children":"Token delivery rate (tokens per second for streaming responses)"}],"\n",["$","li",null,{"children":"Rate limit proximity (estimated from response headers where exposed)"}],"\n"]}],"\n",["$","p",null,{"children":"The load balancer uses these signals to route new requests toward providers with favorable health profiles. It does not use round-robin or static weights; routing is dynamic and updates on every health event."}],"\n",["$","p",null,{"children":"The 23% latency reduction relative to single-provider routing comes from this dynamic selection. The best provider at a given moment — in terms of current queue depth, token delivery rate, and time-to-first-token — varies continuously. Static routing picks a provider and stays there through its bad moments. Dynamic routing moves."}],"\n",["$","$L18",null,{"level":2,"id":"cost-attribution-at-scale","children":"Cost Attribution at Scale"}],"\n",["$","p",null,{"children":"2.8 billion tokens per month across 100+ providers means 100+ different pricing models. Per-input-token, per-output-token, per-request, per-image, per-minute of audio — each provider invoices differently. The gateway normalizes all of this into a per-token attribution that gives downstream cost reporting a consistent unit of account."}],"\n",["$","p",null,{"children":"The attribution system tracks:"}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Raw tokens (input and output separately, since they price differently at every provider)"}],"\n",["$","li",null,{"children":"Model identifier and provider"}],"\n",["$","li",null,{"children":"Request metadata (team, project, API key)"}],"\n",["$","li",null,{"children":"Wall-clock time for latency attribution"}],"\n"]}],"\n",["$","p",null,{"children":"Cost reports are available per API key, per project, and per team through the console. The 31% cost reduction relative to unmanaged single-provider usage comes from a combination of semantic caching (cached requests have near-zero cost), dynamic routing toward price-efficient providers for equivalent quality, and visibility into cost that allows teams to make informed tradeoffs."}],"\n",["$","$L18",null,{"level":2,"id":"the-unified-api-surface","children":"The Unified API Surface"}],"\n",["$","p",null,{"children":"Every capability routes through a single OpenAI-compatible endpoint. Chat completions, embeddings, reranking, image generation, audio transcription, and content moderation all use the same authentication, the same SDKs, and the same cost attribution pipeline. Adding a new provider means registering its adapter in the gateway; consumer code does not change."}],"\n",["$","p",null,{"children":"This composability was the original design goal. The production metrics — 2.8B tokens/month, 99.97% availability, 34% cache hit rate, 23% latency reduction, 31% cost reduction — are the measured outcomes of running that design at scale."}],"\n",["$","$L19",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L1a",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":"# Same code, different model, different provider:"}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"curl"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" https://llm.hanzo.ai/v1/chat/completions"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  -H"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"Authorization: Bearer "}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"$$HANZO_API_KEY"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\""}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  -d"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" '{\"model\": \"zen4\", \"messages\": [...]}'"}]]}]]}]}]}],"\n",["$","p",null,{"children":["The gateway is available at ",["$","$L1b",null,{"href":"https://llm.hanzo.ai","children":"llm.hanzo.ai"}]," and the source is open at ",["$","$L1b",null,{"href":"https://github.com/hanzoai/llm","children":"github.com/hanzoai/llm"}],"."]}]],"className":"prose"}]}]}],["$","div",null,{"className":"mt-10","children":["$","section",null,{"className":"border-t border-border p-0","children":["$","div",null,{"className":"p-6 lg:p-10","children":[["$","h2",null,{"className":"text-2xl font-medium mb-8","children":"Read more"}],["$","div",null,{"className":"flex flex-col gap-8","children":[["$","$L8","/blog/2024-03-12-llm-infrastructure",{"href":"/blog/2024-03-12-llm-infrastructure","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"LLM Infrastructure: Running AI at Scale"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"How we built infrastructure to serve billions of LLM requests for commerce applications."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"March 11, 2024"}]]}]]}],["$","$L8","/blog/2026-02-27-full-stack-ai-agents",{"href":"/blog/2026-02-27-full-stack-ai-agents","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"The Complete AI Agent Stack: Models, Compute, and Tools in One Platform"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Hanzo AI introduces the first platform combining 100+ AI models, cloud compute, GPU access, and 260+ MCP tools under a single developer account."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"February 26, 2026"}]]}]]}],["$","$L8","/blog/2026-02-27-unified-ai-gateway",{"href":"/blog/2026-02-27-unified-ai-gateway","className":"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer","children":["$undefined",["$","div",null,{"className":"space-y-2 flex-1 col-span-1 lg:col-span-8","children":[["$","h3",null,{"className":"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2","children":"One API for Every AI Model: Introducing the Hanzo AI Gateway"}],["$","p",null,{"className":"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4","children":"Hanzo AI launches the industry's first zero-markup multi-provider AI gateway — one API key for 100+ models from every major provider, plus 14 proprietary Zen models."}],["$","time",null,{"className":"block text-xs font-medium text-muted-foreground","children":"February 26, 2026"}]]}]]}]]}]]}]}]}]]}],["$","aside",null,{"className":"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20","children":["$","div",null,{"className":"sticky top-20 space-y-8","children":[["$","div",null,{"className":"flex items-start gap-2","children":["$","div",null,{"className":"flex-1","children":["$","h3",null,{"className":"text-sm tracking-tight text-balance font-semibold","children":"Hanzo AI"}]}]}],["$","div",null,{"className":"border border-border rounded-lg p-6 bg-card","children":["$","$L1c",null,{}]}]]}]}]]}],["$","$L1d",null,{}]]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","content":"black"}]]
b:null
e:{"metadata":[["$","title","0",{"children":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale - Hanzo Blog"}],["$","meta","1",{"name":"description","content":"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."}],["$","link","2",{"rel":"author","href":"https://hanzo.blog"}],["$","meta","3",{"name":"author","content":"Hanzo AI"}],["$","meta","4",{"name":"keywords","content":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale,infrastructure,llm,gateway,production,ai,Hanzo AI,Blog,AI Research,Infrastructure"}],["$","meta","5",{"name":"creator","content":"Hanzo AI"}],["$","meta","6",{"name":"publisher","content":"Hanzo AI"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale"}],["$","meta","10",{"property":"og:title","content":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"}],["$","meta","11",{"property":"og:description","content":"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."}],["$","meta","12",{"property":"og:url","content":"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale"}],["$","meta","13",{"property":"og:site_name","content":"Hanzo Blog"}],["$","meta","14",{"property":"og:image","content":"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image"}],["$","meta","15",{"property":"og:image:width","content":"1200"}],["$","meta","16",{"property":"og:image:height","content":"630"}],["$","meta","17",{"property":"og:image:alt","content":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"}],["$","meta","18",{"property":"og:type","content":"article"}],["$","meta","19",{"property":"article:published_time","content":"2026-02-10"}],["$","meta","20",{"property":"article:author","content":"Hanzo AI"}],["$","meta","21",{"property":"article:tag","content":"infrastructure"}],["$","meta","22",{"property":"article:tag","content":"llm"}],["$","meta","23",{"property":"article:tag","content":"gateway"}],["$","meta","24",{"property":"article:tag","content":"production"}],["$","meta","25",{"property":"article:tag","content":"ai"}],["$","meta","26",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","27",{"name":"twitter:site","content":"@hanzoai"}],["$","meta","28",{"name":"twitter:creator","content":"@hanzoai"}],["$","meta","29",{"name":"twitter:title","content":"2.8 Billion Tokens Per Month: LLM Gateway at Production Scale"}],["$","meta","30",{"name":"twitter:description","content":"The Hanzo LLM Gateway processes 2.8 billion tokens per month at 99.97% availability. Here's how semantic caching, streaming-aware load balancing, and per-provider cost attribution work in production."}],["$","meta","31",{"name":"twitter:image","content":"https://hanzo.blog/blog/2026-02-10-llm-gateway-production-scale/opengraph-image"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$e:metadata","error":null,"digest":"$undefined"}
