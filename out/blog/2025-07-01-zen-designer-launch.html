<!DOCTYPE html><html lang="en" class="__variable_245d8d __variable_97c177 antialiased"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/27834908180db20f-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/78fec81b34c4a365.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/45441016e214994e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-19293e76a0797e5c.js"/><script src="/_next/static/chunks/8b8ad143-e83b7b49f5b24ad0.js" async=""></script><script src="/_next/static/chunks/766-52c0c652dbe24189.js" async=""></script><script src="/_next/static/chunks/main-app-e8025b732b0b8d44.js" async=""></script><script src="/_next/static/chunks/cd24890f-00c67ea9d0bbf445.js" async=""></script><script src="/_next/static/chunks/933-166e0fcc9496c98f.js" async=""></script><script src="/_next/static/chunks/app/layout-d687a3a56998d23e.js" async=""></script><script src="/_next/static/chunks/771-2096a43097a1ac56.js" async=""></script><script src="/_next/static/chunks/698-980099c394b58d6f.js" async=""></script><script src="/_next/static/chunks/218-3bb2d990159730f8.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js" async=""></script><meta name="next-size-adjust" content=""/><meta name="theme-color" content="black"/><title>Zen Designer: 235B Vision-Language Model - Hanzo Blog</title><meta name="description" content="Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."/><link rel="author" href="https://hanzo.blog"/><meta name="author" content="Zach Kelling"/><meta name="keywords" content="Zen Designer: 235B Vision-Language Model,ai,models,zen,vision,multimodal,launch,design,zen-mode,Hanzo AI,Blog,AI Research,Infrastructure"/><meta name="creator" content="Zach Kelling"/><meta name="publisher" content="Hanzo AI"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://hanzo.blog/blog/2025-07-01-zen-designer-launch"/><meta property="og:title" content="Zen Designer: 235B Vision-Language Model"/><meta property="og:description" content="Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."/><meta property="og:url" content="https://hanzo.blog/blog/2025-07-01-zen-designer-launch"/><meta property="og:site_name" content="Hanzo Blog"/><meta property="og:image" content="https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Zen Designer: 235B Vision-Language Model"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-07-01"/><meta property="article:author" content="Zach Kelling"/><meta property="article:tag" content="ai"/><meta property="article:tag" content="models"/><meta property="article:tag" content="zen"/><meta property="article:tag" content="vision"/><meta property="article:tag" content="multimodal"/><meta property="article:tag" content="launch"/><meta property="article:tag" content="design"/><meta property="article:tag" content="zen-mode"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@hanzoai"/><meta name="twitter:creator" content="@hanzoai"/><meta name="twitter:title" content="Zen Designer: 235B Vision-Language Model"/><meta name="twitter:description" content="Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning."/><meta name="twitter:image" content="https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><script>((e,t,r,n,i,o,a,s)=>{let l=document.documentElement,u=["light","dark"];function c(t){var r;(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?i.map(e=>o[e]||e):i;r?(l.classList.remove(...n),l.classList.add(o&&o[t]?o[t]:t)):l.setAttribute(e,t)}),r=t,s&&u.includes(r)&&(l.style.colorScheme=r)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","dark",null,["light","dark"],null,true,true)</script><header class="border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60"><div class="max-w-5xl mx-auto flex items-center justify-between"><a href="https://hanzo.ai" class="flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity"><img alt="Hanzo" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="dark:invert-0 invert" style="color:transparent" src="/hanzo-logo.svg"/><span class="font-semibold text-base tracking-tight">hanzo</span><span class="text-muted-foreground text-sm">/ blog</span></a><nav class="flex items-center gap-4 text-sm text-muted-foreground"><a href="https://hanzo.ai" class="hover:text-foreground transition-colors">hanzo.ai</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay</a><a href="https://discord.gg/hanzo" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle h-3.5 w-3.5" aria-hidden="true"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg>Discord</a></nav></div></header><div class="min-h-screen bg-background relative"><div class="absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]"><div class="absolute top-0 left-0 size-full"><canvas class="pointer-events-none" style="width:0;height:0"></canvas></div></div><div class="space-y-4 border-b border-border relative z-10"><div class="max-w-7xl mx-auto flex flex-col gap-6 p-6"><div class="flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground"><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[&gt;svg]:px-3 h-6 w-6" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg><span class="sr-only">Back to all articles</span></a><div class="flex flex-wrap gap-3 text-muted-foreground"><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">ai</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">models</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">zen</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">vision</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">multimodal</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">launch</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">design</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">zen-mode</span></div><time class="font-medium text-muted-foreground">June 30, 2025</time></div><h1 class="text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance">Zen Designer: 235B Vision-Language Model</h1><p class="text-muted-foreground max-w-4xl md:text-lg md:text-balance">Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning.</p></div></div><div class="flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10"><div class="absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"></div><main class="w-full p-0 overflow-hidden"><div class="p-6 lg:p-10"><div class="prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg"><div class="prose"><h1 id="zen-designer-235b-vision-language-model" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Zen Designer: 235B Vision-Language Model<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Zen Designer is a 235B mixture-of-experts vision-language model built for visual understanding and design reasoning. 22B parameters active per forward pass, with native support for images, video, OCR across 32 languages, and layout analysis.</p>
<h2 id="architecture" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Architecture<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>235B total parameters across a sparse MoE architecture, with 22B activated per token. The vision encoder processes images up to 4K resolution with no degradation on dense visual inputs -- detailed charts, fine text, complex diagrams. The language model handles visual context natively rather than late-fusing separate embeddings.</p>
<h2 id="benchmark-results" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Benchmark Results<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Benchmark</th><th>Score</th><th>Task Type</th></tr></thead><tbody><tr><td>DocVQA</td><td><strong>94.1</strong></td><td>Document question answering</td></tr><tr><td>ChartQA</td><td><strong>88.3</strong></td><td>Chart and graph understanding</td></tr><tr><td>MMBench</td><td><strong>87.6</strong></td><td>Multimodal reasoning</td></tr><tr><td>TextVQA</td><td><strong>85.2</strong></td><td>Text recognition in natural images</td></tr><tr><td>MathVista</td><td><strong>74.8</strong></td><td>Visual math problem solving</td></tr></tbody></table></div>
<p>DocVQA at 94.1 measures the ability to answer questions from scanned documents, invoices, forms, and PDFs. This requires combining OCR, spatial understanding, and reasoning -- not just recognizing text but understanding what it means in context.</p>
<h2 id="ocr-in-32-languages" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">OCR in 32 Languages<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Text recognition in 32 languages with full Unicode handling:</p>
<p><strong>Western</strong>: English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Polish, Czech, Swedish, Norwegian, Finnish, Danish, Romanian</p>
<p><strong>East Asian</strong>: Chinese Simplified, Chinese Traditional, Japanese, Korean</p>
<p><strong>Southeast Asian</strong>: Thai, Vietnamese, Indonesian</p>
<p><strong>Other</strong>: Arabic, Hebrew, Hindi, Bengali, Turkish, Greek, Hungarian, Croatian, Catalan, Ukrainian</p>
<p>OCR quality is consistent across languages -- not an English-first model with degraded support elsewhere. Training explicitly included high-density multilingual document corpora.</p>
<h2 id="capabilities" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Capabilities<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<h3 id="image-analysis" class="scroll-mt-20">Image Analysis</h3>
<p>Natural-language description of any image at multiple levels of detail. Zen Designer understands objects, spatial relationships, text, color, composition, and intent. Ask &quot;what is wrong with this UI&quot; and it will identify specific layout problems, contrast issues, and accessibility concerns.</p>
<h3 id="video-understanding" class="scroll-mt-20">Video Understanding</h3>
<p>Frame-by-frame temporal analysis. Zen Designer processes video as a sequence of visual tokens, maintaining temporal context across frames. Use cases include product demo analysis, design review recordings, and visual QA testing.</p>
<h3 id="text-to-layout" class="scroll-mt-20">Text-to-Layout</h3>
<p>Describe a layout in natural language and Zen Designer generates structured layout specifications -- bounding boxes, component hierarchies, spacing rules. Useful for bridging design intent to implementation.</p>
<h3 id="visual-reasoning" class="scroll-mt-20">Visual Reasoning</h3>
<p>The model reasons about what it sees, not just describes it. Questions like &quot;which chart shows the highest growth rate after Q3&quot; or &quot;which of these wireframes better follows accessibility guidelines&quot; get reasoned answers, not just observations.</p>
<h3 id="design-analysis" class="scroll-mt-20">Design Analysis</h3>
<p>Zen Designer understands design systems: component relationships, visual hierarchy, typography, grid alignment, color theory. It can audit a screenshot against a design specification or identify deviations from a style guide.</p>
<h2 id="get-zen-designer" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Get Zen Designer<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<ul>
<li><strong>HuggingFace</strong>: <a href="https://huggingface.co/zenlm" rel="noreferrer noopener" target="_blank">huggingface.co/zenlm</a></li>
<li><strong>Hanzo Cloud API</strong>: <code>api.hanzo.ai/v1/chat/completions</code> -- model <code>zen-designer</code></li>
<li><strong>Zen LM</strong>: <a href="https://zenlm.org" rel="noreferrer noopener" target="_blank">zenlm.org</a> -- vision API guides and multimodal examples</li>
</ul>
<hr/>
<p><em>Zach Kelling is the founder of Hanzo AI, Techstars &#x27;17.</em></p></div></div></div><div class="mt-10"><section class="border-t border-border p-0"><div class="p-6 lg:p-10"><h2 class="text-2xl font-medium mb-8">Read more</h2><div class="flex flex-col gap-8"><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2024-07-20-zen-vision-launch"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Zen Vision: Multimodal Understanding at 72B Scale</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction.</p><time class="block text-xs font-medium text-muted-foreground">July 19, 2024</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2025-08-01-zen-omni-launch"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Zen Omni: Unified Multimodal AI</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency.</p><time class="block text-xs font-medium text-muted-foreground">July 31, 2025</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2025-10-10-zen-audit-launch"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Zen Audit: Code Security and Smart Contract Analysis</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Zen Audit is trained on CVE databases, audit reports, and vulnerability research to provide automated code security analysis and smart contract auditing with low false positive rates.</p><time class="block text-xs font-medium text-muted-foreground">October 9, 2025</time></div></a></div></div></section></div></main><aside class="hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20"><div class="sticky top-20 space-y-8"><div class="flex items-start gap-2"><div class="flex-1"><h3 class="text-sm tracking-tight text-balance font-semibold">Zach Kelling</h3></div></div><div class="border border-border rounded-lg p-6 bg-card"></div></div></aside></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-«R2nnelb»" data-state="closed" class="lg:hidden fixed bottom-6 right-6 z-50 bg-primary text-primary-foreground p-3 rounded-full shadow-lg hover:bg-primary/90 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-list" aria-hidden="true"><path d="M3 12h.01"></path><path d="M3 18h.01"></path><path d="M3 6h.01"></path><path d="M8 12h13"></path><path d="M8 18h13"></path><path d="M8 6h13"></path></svg></button></div><!--$--><!--/$--><footer class="border-t border-border/50 px-6 py-6 mt-auto"><div class="max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground"><div class="flex items-center gap-2"><img alt="Hanzo" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" class="dark:invert-0 invert opacity-50" style="color:transparent" src="/hanzo-logo.svg"/><span>© 2025 Hanzo AI, Inc. Techstars &#x27;17.</span></div><div class="flex items-center gap-4"><a href="https://hanzo.ai/privacy" class="hover:text-foreground transition-colors">Privacy</a><a href="https://hanzo.ai/terms" class="hover:text-foreground transition-colors">Terms</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo blog</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen blog</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay.blog</a><a href="https://github.com/hanzoai" target="_blank" rel="noopener noreferrer" class="hover:text-foreground transition-colors">GitHub</a></div></div></footer><script src="/_next/static/chunks/webpack-19293e76a0797e5c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[861,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"PostHogProvider\"]\n3:I[2710,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"ThemeProvider\"]\n4:I[9933,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"Image\"]\n5:I[3977,[],\"\"]\n6:I[8765,[],\"\"]\n7:I[4526,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"FlickeringGrid\"]\n8:I[1964,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"\"]\na:I[2463,[],\"OutletBoundary\"]\nd:I[373,[],\"AsyncMetadataOutlet\"]\nf:I[2463,[],\"ViewportBoundary\"]\n11:I[2463,[],\"MetadataBoundary\"]\n13:I[680,[],\"\"]\n:HL[\"/_next/static/media/27834908180db20f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/78fec81b34c4a365.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/45441016e214994e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yeRLKKIdrV-2cpcPU6Jw-\",\"p\":\"\",\"c\":[\"\",\"blog\",\"2025-07-01-zen-designer-launch\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2025-07-01-zen-designer-launch\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/45441016e214994e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_245d8d __variable_97c177 antialiased\",\"suppressHydrationWarning\":true,\"children\":[\"$undefined\",[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"attribute\":\"class\",\"defaultTheme\":\"dark\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[[\"$\",\"header\",null,{\"className\":\"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex items-center justify-between\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":20,\"height\":20,\"className\":\"dark:invert-0 invert\"}],[\"$\",\"span\",null,{\"className\":\"font-semibold text-base tracking-tight\",\"children\":\"hanzo\"}],[\"$\",\"span\",null,{\"className\":\"text-muted-foreground text-sm\",\"children\":\"/ blog\"}]]}],[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"hanzo.ai\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay\"}],[\"$\",\"a\",null,{\"href\":\"https://discord.gg/hanzo\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-message-circle h-3.5 w-3.5\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"vv11sd\",{\"d\":\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"}],\"$undefined\"]}],\"Discord\"]}]]}]]}]}],[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background flex items-center justify-center w-full z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.5,\"flickerChance\":0.1}]}],[\"$\",\"div\",null,{\"className\":\"text-center flex flex-col gap-4 max-w-xs mx-auto relative\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-8xl font-mono font-bold drop-shadow-lg text-primary\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance\",\"children\":\"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL.\"}],[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":\"Back to Home\",\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[\u003esvg]:px-3 w-full rounded-lg h-9 drop-shadow-lg\",\"ref\":null}]]}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"className\":\"border-t border-border/50 px-6 py-6 mt-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":16,\"height\":16,\"className\":\"dark:invert-0 invert opacity-50\"}],[\"$\",\"span\",null,{\"children\":\"© 2025 Hanzo AI, Inc. Techstars '17.\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/privacy\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Privacy\"}],[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/terms\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Terms\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay.blog\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/hanzoai\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"GitHub\"}]]}]]}]}]]}]}]}]]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"2025-07-01-zen-designer-launch\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L9\",null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"EQXcsGawoR0jVRCEEjCJWv\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$L11\",null,{\"children\":\"$L12\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"14:\"$Sreact.suspense\"\n15:I[373,[],\"AsyncMetadata\"]\n17:I[9048,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"HashScrollHandler\"]\n18:I[4170,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"CopyHeader\"]\n19:I[3979,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"default\"]\n1a:I[9372,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"TableOfContents\"]\n1b:I[1413,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"MobileTableOfContents\"]\n12:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$14\",null,{\"fallback\":null,\"children\":[\"$\",\"$L15\",null,{\"promise\":\"$@16\"}]}]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background relative\",\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.2,\"flickerChance\":0.05}]}],[\"$\",\"div\",null,{\"className\":\"space-y-4 border-b border-border relative z-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto flex flex-col gap-6 p-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground\",\"children\":[[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left w-4 h-4\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Back to all articles\"}]],\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[\u003esvg]:px-3 h-6 w-6\",\"ref\":null}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 text-muted-foreground\",\"children\":[[\"$\",\"span\",\"ai\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"ai\"}],[\"$\",\"span\",\"models\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"models\"}],[\"$\",\"span\",\"zen\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"zen\"}],[\"$\",\"span\",\"vision\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"vision\"}],[\"$\",\"span\",\"multimodal\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"multimodal\"}],[\"$\",\"span\",\"launch\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"launch\"}],[\"$\",\"span\",\"design\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"design\"}],[\"$\",\"span\",\"zen-mode\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"zen-mode\"}]]}],[\"$\",\"time\",null,{\"className\":\"font-medium text-muted-foreground\",\"children\":\"June 30, 2025\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance\",\"children\":\"Zen Designer: 235B Vision-Language Model\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground max-w-4xl md:text-lg md:text-balance\",\"children\":\"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none\"}],[\"$\",\"main\",null,{\"className\":\"w-full p-0 overflow-hidden\",\"children\":[null,[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg\",\"children\":[\"$\",\"div\",null,{\"ref\":\"$undefined\",\"children\":[[\"$\",\"$L18\",null,{\"level\":1,\"id\":\"zen-designer-235b-vision-language-model\",\"children\":\"Zen Designer: 235B Vision-Language Model\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Zen Designer is a 235B mixture-of-experts vision-language model built for visual understanding and design reasoning. 22B parameters active per forward pass, with native support for images, video, OCR across 32 languages, and layout analysis.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"architecture\",\"children\":\"Architecture\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"235B total parameters across a sparse MoE architecture, with 22B activated per token. The vision encoder processes images up to 4K resolution with no degradation on dense visual inputs -- detailed charts, fine text, complex diagrams. The language model handles visual context natively rather than late-fusing separate embeddings.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"benchmark-results\",\"children\":\"Benchmark Results\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Benchmark\"}],[\"$\",\"th\",null,{\"children\":\"Score\"}],[\"$\",\"th\",null,{\"children\":\"Task Type\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"DocVQA\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"94.1\"}]}],[\"$\",\"td\",null,{\"children\":\"Document question answering\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ChartQA\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.3\"}]}],[\"$\",\"td\",null,{\"children\":\"Chart and graph understanding\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMBench\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"87.6\"}]}],[\"$\",\"td\",null,{\"children\":\"Multimodal reasoning\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TextVQA\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"85.2\"}]}],[\"$\",\"td\",null,{\"children\":\"Text recognition in natural images\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MathVista\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.8\"}]}],[\"$\",\"td\",null,{\"children\":\"Visual math problem solving\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"DocVQA at 94.1 measures the ability to answer questions from scanned documents, invoices, forms, and PDFs. This requires combining OCR, spatial understanding, and reasoning -- not just recognizing text but understanding what it means in context.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"ocr-in-32-languages\",\"children\":\"OCR in 32 Languages\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Text recognition in 32 languages with full Unicode handling:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Western\"}],\": English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Polish, Czech, Swedish, Norwegian, Finnish, Danish, Romanian\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"East Asian\"}],\": Chinese Simplified, Chinese Traditional, Japanese, Korean\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Southeast Asian\"}],\": Thai, Vietnamese, Indonesian\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Other\"}],\": Arabic, Hebrew, Hindi, Bengali, Turkish, Greek, Hungarian, Croatian, Catalan, Ukrainian\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"OCR quality is consistent across languages -- not an English-first model with degraded support elsewhere. Training explicitly included high-density multilingual document corpora.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"capabilities\",\"children\":\"Capabilities\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":3,\"id\":\"image-analysis\",\"children\":\"Image Analysis\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Natural-language description of any image at multiple levels of detail. Zen Designer understands objects, spatial relationships, text, color, composition, and intent. Ask \\\"what is wrong with this UI\\\" and it will identify specific layout problems, contrast issues, and accessibility concerns.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":3,\"id\":\"video-understanding\",\"children\":\"Video Understanding\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Frame-by-frame temporal analysis. Zen Designer processes video as a sequence of visual tokens, maintaining temporal context across frames. Use cases include product demo analysis, design review recordings, and visual QA testing.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":3,\"id\":\"text-to-layout\",\"children\":\"Text-to-Layout\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Describe a layout in natural language and Zen Designer generates structured layout specifications -- bounding boxes, component hierarchies, spacing rules. Useful for bridging design intent to implementation.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":3,\"id\":\"visual-reasoning\",\"children\":\"Visual Reasoning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The model reasons about what it sees, not just describes it. Questions like \\\"which chart shows the highest growth rate after Q3\\\" or \\\"which of these wireframes better follows accessibility guidelines\\\" get reasoned answers, not just observations.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":3,\"id\":\"design-analysis\",\"children\":\"Design Analysis\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Zen Designer understands design systems: component relationships, visual hierarchy, typography, grid alignment, color theory. It can audit a screenshot against a design specification or identify deviations from a style guide.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"get-zen-designer\",\"children\":\"Get Zen Designer\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"HuggingFace\"}],\": \",[\"$\",\"$L19\",null,{\"href\":\"https://huggingface.co/zenlm\",\"children\":\"huggingface.co/zenlm\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Hanzo Cloud API\"}],\": \",[\"$\",\"code\",null,{\"children\":\"api.hanzo.ai/v1/chat/completions\"}],\" -- model \",[\"$\",\"code\",null,{\"children\":\"zen-designer\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Zen LM\"}],\": \",[\"$\",\"$L19\",null,{\"href\":\"https://zenlm.org\",\"children\":\"zenlm.org\"}],\" -- vision API guides and multimodal examples\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Zach Kelling is the founder of Hanzo AI, Techstars '17.\"}]}]],\"className\":\"prose\"}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-10\",\"children\":[\"$\",\"section\",null,{\"className\":\"border-t border-border p-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium mb-8\",\"children\":\"Read more\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-8\",\"children\":[[\"$\",\"$L8\",\"/blog/2024-07-20-zen-vision-launch\",{\"href\":\"/blog/2024-07-20-zen-vision-launch\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Zen Vision: Multimodal Understanding at 72B Scale\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Zen Vision brings 72B-parameter visual understanding to the Zen model family, with strong performance on OCR, diagram reasoning, screenshot analysis, and document extraction.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"July 19, 2024\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2025-08-01-zen-omni-launch\",{\"href\":\"/blog/2025-08-01-zen-omni-launch\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Zen Omni: Unified Multimodal AI\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Zen Omni is a 30B MoE unified multimodal model with Thinker-Talker architecture, handling text, vision, and audio in a single model with real-time speech-to-speech at under 300ms latency.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"July 31, 2025\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2025-10-10-zen-audit-launch\",{\"href\":\"/blog/2025-10-10-zen-audit-launch\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Zen Audit: Code Security and Smart Contract Analysis\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Zen Audit is trained on CVE databases, audit reports, and vulnerability research to provide automated code security analysis and smart contract auditing with low false positive rates.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"October 9, 2025\"}]]}]]}]]}]]}]}]}]]}],[\"$\",\"aside\",null,{\"className\":\"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20\",\"children\":[\"$\",\"div\",null,{\"className\":\"sticky top-20 space-y-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-start gap-2\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-sm tracking-tight text-balance font-semibold\",\"children\":\"Zach Kelling\"}]}]}],[\"$\",\"div\",null,{\"className\":\"border border-border rounded-lg p-6 bg-card\",\"children\":[\"$\",\"$L1a\",null,{}]}]]}]}]]}],[\"$\",\"$L1b\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"black\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Zen Designer: 235B Vision-Language Model - Hanzo Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"author\",\"href\":\"https://hanzo.blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"author\",\"content\":\"Zach Kelling\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"Zen Designer: 235B Vision-Language Model,ai,models,zen,vision,multimodal,launch,design,zen-mode,Hanzo AI,Blog,AI Research,Infrastructure\"}],[\"$\",\"meta\",\"5\",{\"name\":\"creator\",\"content\":\"Zach Kelling\"}],[\"$\",\"meta\",\"6\",{\"name\":\"publisher\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"7\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"8\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"9\",{\"rel\":\"canonical\",\"href\":\"https://hanzo.blog/blog/2025-07-01-zen-designer-launch\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"Zen Designer: 235B Vision-Language Model\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://hanzo.blog/blog/2025-07-01-zen-designer-launch\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:site_name\",\"content\":\"Hanzo Blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image\",\"content\":\"https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:image:alt\",\"content\":\"Zen Designer: 235B Vision-Language Model\"}],[\"$\",\"meta\",\"18\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:published_time\",\"content\":\"2025-07-01\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:author\",\"content\":\"Zach Kelling\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"ai\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"models\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"zen\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"vision\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"multimodal\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"launch\"}],[\"$\",\"meta\",\"27\",{\"property\":\"article:tag\",\"content\":\"design\"}],[\"$\",\"meta\",\"28\",{\"property\":\"article:tag\",\"content\":\"zen-mode\"}],[\"$\",\"meta\",\"29\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:site\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:creator\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:title\",\"content\":\"Zen Designer: 235B Vision-Language Model\"}],[\"$\",\"meta\",\"33\",{\"name\":\"twitter:description\",\"content\":\"Zen Designer is a 235B MoE vision-language model with 22B active parameters, supporting image analysis, video understanding, OCR in 32 languages, and native design reasoning.\"}],[\"$\",\"meta\",\"34\",{\"name\":\"twitter:image\",\"content\":\"https://hanzo.blog/blog/2025-07-01-zen-designer-launch/opengraph-image\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"16:{\"metadata\":\"$e:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>