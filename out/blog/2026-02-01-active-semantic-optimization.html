<!DOCTYPE html><html lang="en" class="__variable_245d8d __variable_97c177 antialiased"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/27834908180db20f-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/78fec81b34c4a365.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/45441016e214994e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-19293e76a0797e5c.js"/><script src="/_next/static/chunks/8b8ad143-e83b7b49f5b24ad0.js" async=""></script><script src="/_next/static/chunks/766-52c0c652dbe24189.js" async=""></script><script src="/_next/static/chunks/main-app-e8025b732b0b8d44.js" async=""></script><script src="/_next/static/chunks/cd24890f-00c67ea9d0bbf445.js" async=""></script><script src="/_next/static/chunks/933-166e0fcc9496c98f.js" async=""></script><script src="/_next/static/chunks/app/layout-d687a3a56998d23e.js" async=""></script><script src="/_next/static/chunks/771-2096a43097a1ac56.js" async=""></script><script src="/_next/static/chunks/698-980099c394b58d6f.js" async=""></script><script src="/_next/static/chunks/218-3bb2d990159730f8.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js" async=""></script><meta name="next-size-adjust" content=""/><meta name="theme-color" content="black"/><title>Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning - Hanzo Blog</title><meta name="description" content="ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified."/><link rel="author" href="https://hanzo.blog"/><meta name="author" content="Hanzo AI"/><meta name="keywords" content="Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning,ai,ml,optimization,research,grpo,paper,Hanzo AI,Blog,AI Research,Infrastructure"/><meta name="creator" content="Hanzo AI"/><meta name="publisher" content="Hanzo AI"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><link rel="canonical" href="https://hanzo.blog/blog/2026-02-01-active-semantic-optimization"/><meta property="og:title" content="Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning"/><meta property="og:description" content="ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified."/><meta property="og:url" content="https://hanzo.blog/blog/2026-02-01-active-semantic-optimization"/><meta property="og:site_name" content="Hanzo Blog"/><meta property="og:image" content="https://hanzo.blog/blog/2026-02-01-active-semantic-optimization/opengraph-image"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2026-02-01"/><meta property="article:author" content="Hanzo AI"/><meta property="article:tag" content="ai"/><meta property="article:tag" content="ml"/><meta property="article:tag" content="optimization"/><meta property="article:tag" content="research"/><meta property="article:tag" content="grpo"/><meta property="article:tag" content="paper"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@hanzoai"/><meta name="twitter:creator" content="@hanzoai"/><meta name="twitter:title" content="Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning"/><meta name="twitter:description" content="ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified."/><meta name="twitter:image" content="https://hanzo.blog/blog/2026-02-01-active-semantic-optimization/opengraph-image"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><div hidden=""><!--$--><!--/$--></div><script>((e,t,r,n,i,o,a,s)=>{let l=document.documentElement,u=["light","dark"];function c(t){var r;(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?i.map(e=>o[e]||e):i;r?(l.classList.remove(...n),l.classList.add(o&&o[t]?o[t]:t)):l.setAttribute(e,t)}),r=t,s&&u.includes(r)&&(l.style.colorScheme=r)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","dark",null,["light","dark"],null,true,true)</script><header class="border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60"><div class="max-w-5xl mx-auto flex items-center justify-between"><a href="https://hanzo.ai" class="flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity"><img alt="Hanzo" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="dark:invert-0 invert" style="color:transparent" src="/hanzo-logo.svg"/><span class="font-semibold text-base tracking-tight">hanzo</span><span class="text-muted-foreground text-sm">/ blog</span></a><nav class="flex items-center gap-4 text-sm text-muted-foreground"><a href="https://hanzo.ai" class="hover:text-foreground transition-colors">hanzo.ai</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay</a><a href="https://discord.gg/hanzo" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-message-circle h-3.5 w-3.5" aria-hidden="true"><path d="M7.9 20A9 9 0 1 0 4 16.1L2 22Z"></path></svg>Discord</a></nav></div></header><div class="min-h-screen bg-background relative"><div class="absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]"><div class="absolute top-0 left-0 size-full"><canvas class="pointer-events-none" style="width:0;height:0"></canvas></div></div><div class="space-y-4 border-b border-border relative z-10"><div class="max-w-7xl mx-auto flex flex-col gap-6 p-6"><div class="flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground"><a class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg:not([class*=&#x27;size-&#x27;])]:size-4 shrink-0 [&amp;_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[&gt;svg]:px-3 h-6 w-6" href="/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-left w-4 h-4" aria-hidden="true"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg><span class="sr-only">Back to all articles</span></a><div class="flex flex-wrap gap-3 text-muted-foreground"><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">ai</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">ml</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">optimization</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">research</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">grpo</span><span class="h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center">paper</span></div><time class="font-medium text-muted-foreground">January 31, 2026</time></div><h1 class="text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance">Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning</h1><p class="text-muted-foreground max-w-4xl md:text-lg md:text-balance">ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified.</p></div></div><div class="flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10"><div class="absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none"></div><main class="w-full p-0 overflow-hidden"><div class="p-6 lg:p-10"><div class="prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg"><div class="prose"><h1 id="active-semantic-optimization-18-adaptation-vs-10000-fine-tuning" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Fine-tuning a language model costs money proportional to the number of parameters you update, the size of your dataset, and the compute required for gradient descent. For LoRA/QLoRA on a 7B model: $10,000–$50,000. For full RLHF with human preference data on a frontier model: $100,000 or more. These costs gate adaptation behind a funding threshold that most teams cannot clear.</p>
<p>Active Semantic Optimization (ASO) achieves comparable gains at decode time, without touching model weights. The cost for a representative adaptation task: approximately $18 in inference compute.</p>
<h2 id="training-free-grpo" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Training-Free GRPO<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The theoretical foundation is Training-Free Group-Relative Policy Optimization (TF-GRPO). Standard GRPO uses group-relative rewards to estimate policy gradients and update model weights. TF-GRPO retains the group-relative scoring structure but applies the resulting preference signal at decode time rather than back-propagating it through the network.</p>
<p>The key insight: if you can compute a quality signal over a group of candidate completions, you can use that signal to weight a mixture of decoding distributions without ever updating a parameter. The model itself is frozen. The adaptation lives in the decoding layer.</p>
<h2 id="product-of-experts-ensemble" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Product-of-Experts Ensemble<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>ASO constructs a product-of-experts (PoE) ensemble at decode time. Multiple decoding processes run in parallel over the same prompt, each with different sampling parameters or prompted with different chain-of-thought prefixes. Their output distributions are combined multiplicatively:</p>
<figure dir="ltr" class="my-4 rounded-xl bg-fd-card p-1 shiki relative border outline-none not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="0"><div class="empty:hidden absolute top-1 right-1 z-2 bg-fd-card rounded-bl-lg border-l border-b text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md p-2 text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none hover:bg-fd-accent hover:text-fd-accent-foreground [&amp;_svg]:size-3.5" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div><div class="bg-fd-secondary rounded-lg border text-[13px] py-3.5 overflow-auto max-h-[600px] fd-scroll-container" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span>p_combined(x) ∝ ∏_m p_m(x)^(η_m)</span></span></code></pre></div></figure>
<p>The expert weights η_m are not hand-tuned. They are derived in closed form from quality attestation scores:</p>
<figure dir="ltr" class="my-4 rounded-xl bg-fd-card p-1 shiki relative border outline-none not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="0"><div class="empty:hidden absolute top-1 right-1 z-2 bg-fd-card rounded-bl-lg border-l border-b text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md p-2 text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none hover:bg-fd-accent hover:text-fd-accent-foreground [&amp;_svg]:size-3.5" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div><div class="bg-fd-secondary rounded-lg border text-[13px] py-3.5 overflow-auto max-h-[600px] fd-scroll-container" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span>η_m ∝ q_m / (1 - q_m)</span></span></code></pre></div></figure>
<p>where q_m is the quality attestation reliability for expert m — a signal computed from group-relative scoring of recent outputs. Experts with higher demonstrated quality receive exponentially more weight in the combined distribution. This is the same log-odds weighting used in boosting, applied to decoding.</p>
<h2 id="verified-results" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Verified Results<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>We evaluated ASO against baseline models on three benchmarks:</p>
<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Benchmark</th><th>Baseline</th><th>ASO</th><th>Gain</th></tr></thead><tbody><tr><td>HumanEval</td><td>63.7%</td><td>72.8%</td><td>+9.1 pp</td></tr><tr><td>MBPP</td><td>61.2%</td><td>68.4%</td><td>+7.2 pp</td></tr><tr><td>SWE-bench Verified</td><td>12.5%</td><td>18.2%</td><td>+5.7 pp</td></tr></tbody></table></div>
<p>These are the numbers from the actual evaluation runs. No cherry-picking; these are the primary benchmark results reported in the ASO paper.</p>
<h2 id="1-bit-semantic-compression" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">1-Bit Semantic Compression<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The quality attestations computed during ASO runs can be stored and reused. We compress them using 1-bit semantic quantization: each attestation is reduced to a single bit indicating above- or below-median quality for its group. This achieves 29.5× storage savings relative to full-precision scoring while preserving sufficient signal for weight computation.</p>
<p>Stored attestations form a semantic prior that can be loaded at the start of future runs, bootstrapping the PoE weighting without cold-starting. This is the ASO equivalent of a pre-trained adapter — except it requires no gradient computation to produce.</p>
<h2 id="cost-comparison" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Cost Comparison<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Method</th><th>Approximate Cost</th><th>Modifies Weights</th></tr></thead><tbody><tr><td>ASO</td><td>~$18</td><td>No</td></tr><tr><td>LoRA/QLoRA (7B)</td><td>$10,000–$50,000</td><td>Yes</td></tr><tr><td>Full RLHF</td><td>$100,000+</td><td>Yes</td></tr></tbody></table></div>
<p>The $18 figure is inference compute for generating the candidate groups, computing quality scores, and running the PoE decoding. No GPU rental for training, no dataset curation, no checkpoint storage at training scale.</p>
<h2 id="what-this-does-not-replace" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">What This Does Not Replace<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>ASO is not a substitute for fine-tuning in every case. If you need a model to acquire genuinely new knowledge — factual updates after the training cutoff, proprietary internal terminology, task formats that fall entirely outside the base model&#x27;s distribution — fine-tuning has capabilities that decode-time adaptation cannot replicate.</p>
<p>ASO is optimal when the base model has the latent capability and the problem is eliciting it reliably. For code generation, reasoning, and agentic tasks where frontier models already have relevant priors, $18 of ASO compute gets you most of the way there.</p>
<h2 id="implementation" class="group relative scroll-mt-20 cursor-pointer hover:text-muted-foreground transition-colors duration-200 flex items-center gap-2" title="Click to copy link to this section">Implementation<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link w-4 h-4 opacity-0 group-hover:opacity-100 transition-opacity duration-200 flex-shrink-0" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>ASO is implemented in the <code>zoo/gym</code> training platform and exposed through the Hanzo Agent SDK. The PoE ensemble runs against any OpenAI-compatible endpoint, making it provider-agnostic. Quality attestations are computed locally and stored in the 1-bit compressed format described above.</p>
<p>The full paper is available in <code>zoo/papers/hllm-training-free-grpo.tex</code>.</p></div></div></div><div class="mt-10"><section class="border-t border-border p-0"><div class="p-6 lg:p-10"><h2 class="text-2xl font-medium mb-8">Read more</h2><div class="flex flex-col gap-8"><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2026-02-03-decentralized-semantic-optimization"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Decentralized Semantic Optimization: Byzantine-Robust Knowledge Sharing Across Agents</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">DSO achieves 15.2% improvement when 10 agents share semantic priors versus isolated operation, while remaining resilient to 20% Byzantine participants. Convergence rate: O(1/√n).</p><time class="block text-xs font-medium text-muted-foreground">February 2, 2026</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2026-02-05-hamiltonian-market-maker"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Hamiltonian Market Maker: Physics-Inspired AMM with 32% Lower Impermanent Loss</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">HMM applies Hamiltonian mechanics to AMM design: symplectic integrators preserve geometric structure, conservation laws prevent MEV, and phase space dynamics produce 15.3% higher capital efficiency and 32% lower impermanent loss than Uniswap v3.</p><time class="block text-xs font-medium text-muted-foreground">February 4, 2026</time></div></a><a class="group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer" href="/blog/2024-11-15-zen-science-launch"><div class="space-y-2 flex-1 col-span-1 lg:col-span-8"><h3 class="text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2">Zen Science: AI for Research, Hypothesis Generation, and Literature Synthesis</h3><p class="text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4">Zen Science is trained on a deep scientific corpus including PubMed, arXiv, and patent databases, enabling hypothesis generation, experimental design support, and cross-domain literature synthesis.</p><time class="block text-xs font-medium text-muted-foreground">November 14, 2024</time></div></a></div></div></section></div></main><aside class="hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20"><div class="sticky top-20 space-y-8"><div class="flex items-start gap-2"><div class="flex-1"><h3 class="text-sm tracking-tight text-balance font-semibold">Hanzo AI</h3></div></div><div class="border border-border rounded-lg p-6 bg-card"></div></div></aside></div><button type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-«R2nnelb»" data-state="closed" class="lg:hidden fixed bottom-6 right-6 z-50 bg-primary text-primary-foreground p-3 rounded-full shadow-lg hover:bg-primary/90 transition-colors"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-list" aria-hidden="true"><path d="M3 12h.01"></path><path d="M3 18h.01"></path><path d="M3 6h.01"></path><path d="M8 12h13"></path><path d="M8 18h13"></path><path d="M8 6h13"></path></svg></button></div><!--$--><!--/$--><footer class="border-t border-border/50 px-6 py-6 mt-auto"><div class="max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground"><div class="flex items-center gap-2"><img alt="Hanzo" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" class="dark:invert-0 invert opacity-50" style="color:transparent" src="/hanzo-logo.svg"/><span>© 2025 Hanzo AI, Inc. Techstars &#x27;17.</span></div><div class="flex items-center gap-4"><a href="https://hanzo.ai/privacy" class="hover:text-foreground transition-colors">Privacy</a><a href="https://hanzo.ai/terms" class="hover:text-foreground transition-colors">Terms</a><a href="https://blog.zoo.ngo" class="hover:text-foreground transition-colors hidden sm:block">zoo blog</a><a href="https://zenlm.org/blog" class="hover:text-foreground transition-colors hidden sm:block">zen blog</a><a href="https://zeekay.blog" class="hover:text-foreground transition-colors hidden sm:block">zeekay.blog</a><a href="https://github.com/hanzoai" target="_blank" rel="noopener noreferrer" class="hover:text-foreground transition-colors">GitHub</a></div></div></footer><script src="/_next/static/chunks/webpack-19293e76a0797e5c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[861,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"PostHogProvider\"]\n3:I[2710,[\"803\",\"static/chunks/cd24890f-00c67ea9d0bbf445.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"177\",\"static/chunks/app/layout-d687a3a56998d23e.js\"],\"ThemeProvider\"]\n4:I[9933,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"Image\"]\n5:I[3977,[],\"\"]\n6:I[8765,[],\"\"]\n7:I[4526,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"FlickeringGrid\"]\n8:I[1964,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"\"]\na:I[2463,[],\"OutletBoundary\"]\nd:I[373,[],\"AsyncMetadataOutlet\"]\nf:I[2463,[],\"ViewportBoundary\"]\n11:I[2463,[],\"MetadataBoundary\"]\n13:I[680,[],\"\"]\n:HL[\"/_next/static/media/27834908180db20f-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/78fec81b34c4a365.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/45441016e214994e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yeRLKKIdrV-2cpcPU6Jw-\",\"p\":\"\",\"c\":[\"\",\"blog\",\"2026-02-01-active-semantic-optimization\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2026-02-01-active-semantic-optimization\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/45441016e214994e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_245d8d __variable_97c177 antialiased\",\"suppressHydrationWarning\":true,\"children\":[\"$undefined\",[\"$\",\"body\",null,{\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"attribute\":\"class\",\"defaultTheme\":\"dark\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[[\"$\",\"header\",null,{\"className\":\"border-b border-border/50 px-6 py-4 sticky top-0 z-20 bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex items-center justify-between\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"flex items-center gap-2 text-foreground hover:opacity-80 transition-opacity\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":20,\"height\":20,\"className\":\"dark:invert-0 invert\"}],[\"$\",\"span\",null,{\"className\":\"font-semibold text-base tracking-tight\",\"children\":\"hanzo\"}],[\"$\",\"span\",null,{\"className\":\"text-muted-foreground text-sm\",\"children\":\"/ blog\"}]]}],[\"$\",\"nav\",null,{\"className\":\"flex items-center gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"hanzo.ai\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay\"}],[\"$\",\"a\",null,{\"href\":\"https://discord.gg/hanzo\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-1.5 px-3 py-1.5 rounded-full border border-border text-foreground hover:bg-accent transition-all text-sm font-medium\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-message-circle h-3.5 w-3.5\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"vv11sd\",{\"d\":\"M7.9 20A9 9 0 1 0 4 16.1L2 22Z\"}],\"$undefined\"]}],\"Discord\"]}]]}]]}]}],[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background flex items-center justify-center w-full z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[500px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.5,\"flickerChance\":0.1}]}],[\"$\",\"div\",null,{\"className\":\"text-center flex flex-col gap-4 max-w-xs mx-auto relative\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-8xl font-mono font-bold drop-shadow-lg text-primary\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-base leading-relaxed text-center tracking-tight text-balance\",\"children\":\"Sorry, we couldn't find the page you're looking for. The page might have been moved, deleted, or you entered the wrong URL.\"}],[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":\"Back to Home\",\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive bg-primary text-primary-foreground shadow-xs hover:bg-primary/90 px-4 py-2 has-[\u003esvg]:px-3 w-full rounded-lg h-9 drop-shadow-lg\",\"ref\":null}]]}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"className\":\"border-t border-border/50 px-6 py-6 mt-auto\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-5xl mx-auto flex flex-col sm:flex-row items-center justify-between gap-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2\",\"children\":[[\"$\",\"$L4\",null,{\"src\":\"/hanzo-logo.svg\",\"alt\":\"Hanzo\",\"width\":16,\"height\":16,\"className\":\"dark:invert-0 invert opacity-50\"}],[\"$\",\"span\",null,{\"children\":\"© 2025 Hanzo AI, Inc. Techstars '17.\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/privacy\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Privacy\"}],[\"$\",\"a\",null,{\"href\":\"https://hanzo.ai/terms\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"Terms\"}],[\"$\",\"a\",null,{\"href\":\"https://blog.zoo.ngo\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zoo blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zenlm.org/blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zen blog\"}],[\"$\",\"a\",null,{\"href\":\"https://zeekay.blog\",\"className\":\"hover:text-foreground transition-colors hidden sm:block\",\"children\":\"zeekay.blog\"}],[\"$\",\"a\",null,{\"href\":\"https://github.com/hanzoai\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"hover:text-foreground transition-colors\",\"children\":\"GitHub\"}]]}]]}]}]]}]}]}]]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"2026-02-01-active-semantic-optimization\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L9\",null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"IaMbeAFB4w_Y1s_LgjKywv\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],[\"$\",\"$L11\",null,{\"children\":\"$L12\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$13\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"14:\"$Sreact.suspense\"\n15:I[373,[],\"AsyncMetadata\"]\n17:I[9048,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"HashScrollHandler\"]\n18:I[4170,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"CopyHeader\"]\n19:I[4255,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"CodeBlock\"]\n1a:I[4255,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"Pre\"]\n1b:I[9372,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"TableOfContents\"]\n1c:I[1413,[\"771\",\"static/chunks/771-2096a43097a1ac56.js\",\"933\",\"static/chunks/933-166e0fcc9496c98f.js\",\"698\",\"static/chunks/698-980099c394b58d6f.js\",\"218\",\"static/chunks/218-3bb2d990159730f8.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-b595d014fe271b46.js\"],\"MobileTableOfContents\"]\n12:[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$14\",null,{\"fallback\":null,\"children\":[\"$\",\"$L15\",null,{\"promise\":\"$@16\"}]}]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background relative\",\"children\":[[\"$\",\"$L17\",null,{}],[\"$\",\"div\",null,{\"className\":\"absolute top-0 left-0 z-0 w-full h-[200px] [mask-image:linear-gradient(to_top,transparent_25%,black_95%)]\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"absolute top-0 left-0 size-full\",\"squareSize\":4,\"gridGap\":6,\"color\":\"#6B7280\",\"maxOpacity\":0.2,\"flickerChance\":0.05}]}],[\"$\",\"div\",null,{\"className\":\"space-y-4 border-b border-border relative z-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto flex flex-col gap-6 p-6\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center gap-3 gap-y-5 text-sm text-muted-foreground\",\"children\":[[\"$\",\"$L8\",null,{\"href\":\"/\",\"children\":[[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-left w-4 h-4\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"className\":\"sr-only\",\"children\":\"Back to all articles\"}]],\"className\":\"inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [\u0026_svg]:pointer-events-none [\u0026_svg:not([class*='size-'])]:size-4 shrink-0 [\u0026_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50 px-4 py-2 has-[\u003esvg]:px-3 h-6 w-6\",\"ref\":null}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 text-muted-foreground\",\"children\":[[\"$\",\"span\",\"ai\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"ai\"}],[\"$\",\"span\",\"ml\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"ml\"}],[\"$\",\"span\",\"optimization\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"optimization\"}],[\"$\",\"span\",\"research\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"research\"}],[\"$\",\"span\",\"grpo\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"grpo\"}],[\"$\",\"span\",\"paper\",{\"className\":\"h-6 w-fit px-3 text-sm font-medium bg-muted text-muted-foreground rounded-md border flex items-center justify-center\",\"children\":\"paper\"}]]}],[\"$\",\"time\",null,{\"className\":\"font-medium text-muted-foreground\",\"children\":\"January 31, 2026\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl lg:text-6xl font-medium tracking-tighter text-balance\",\"children\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground max-w-4xl md:text-lg md:text-balance\",\"children\":\"ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"flex divide-x divide-border relative max-w-7xl mx-auto px-4 md:px-0 z-10\",\"children\":[[\"$\",\"div\",null,{\"className\":\"absolute max-w-7xl mx-auto left-1/2 -translate-x-1/2 w-[calc(100%-2rem)] lg:w-full h-full border-x border-border p-0 pointer-events-none\"}],[\"$\",\"main\",null,{\"className\":\"w-full p-0 overflow-hidden\",\"children\":[null,[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none prose-headings:scroll-mt-8 prose-headings:font-semibold prose-a:no-underline prose-headings:tracking-tight prose-headings:text-balance prose-p:tracking-tight prose-p:text-balance prose-lg\",\"children\":[\"$\",\"div\",null,{\"ref\":\"$undefined\",\"children\":[[\"$\",\"$L18\",null,{\"level\":1,\"id\":\"active-semantic-optimization-18-adaptation-vs-10000-fine-tuning\",\"children\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Fine-tuning a language model costs money proportional to the number of parameters you update, the size of your dataset, and the compute required for gradient descent. For LoRA/QLoRA on a 7B model: $10,000–$50,000. For full RLHF with human preference data on a frontier model: $100,000 or more. These costs gate adaptation behind a funding threshold that most teams cannot clear.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Active Semantic Optimization (ASO) achieves comparable gains at decode time, without touching model weights. The cost for a representative adaptation task: approximately $18 in inference compute.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"training-free-grpo\",\"children\":\"Training-Free GRPO\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The theoretical foundation is Training-Free Group-Relative Policy Optimization (TF-GRPO). Standard GRPO uses group-relative rewards to estimate policy gradients and update model weights. TF-GRPO retains the group-relative scoring structure but applies the resulting preference signal at decode time rather than back-propagating it through the network.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The key insight: if you can compute a quality signal over a group of candidate completions, you can use that signal to weight a mixture of decoding distributions without ever updating a parameter. The model itself is frozen. The adaptation lives in the decoding layer.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"product-of-experts-ensemble\",\"children\":\"Product-of-Experts Ensemble\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"ASO constructs a product-of-experts (PoE) ensemble at decode time. Multiple decoding processes run in parallel over the same prompt, each with different sampling parameters or prompted with different chain-of-thought prefixes. Their output distributions are combined multiplicatively:\"}],\"\\n\",[\"$\",\"$L19\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L1a\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"p_combined(x) ∝ ∏_m p_m(x)^(η_m)\"}]}]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The expert weights η_m are not hand-tuned. They are derived in closed form from quality attestation scores:\"}],\"\\n\",[\"$\",\"$L19\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L1a\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"η_m ∝ q_m / (1 - q_m)\"}]}]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"where q_m is the quality attestation reliability for expert m — a signal computed from group-relative scoring of recent outputs. Experts with higher demonstrated quality receive exponentially more weight in the combined distribution. This is the same log-odds weighting used in boosting, applied to decoding.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"verified-results\",\"children\":\"Verified Results\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We evaluated ASO against baseline models on three benchmarks:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Benchmark\"}],[\"$\",\"th\",null,{\"children\":\"Baseline\"}],[\"$\",\"th\",null,{\"children\":\"ASO\"}],[\"$\",\"th\",null,{\"children\":\"Gain\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"63.7%\"}],[\"$\",\"td\",null,{\"children\":\"72.8%\"}],[\"$\",\"td\",null,{\"children\":\"+9.1 pp\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"61.2%\"}],[\"$\",\"td\",null,{\"children\":\"68.4%\"}],[\"$\",\"td\",null,{\"children\":\"+7.2 pp\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"SWE-bench Verified\"}],[\"$\",\"td\",null,{\"children\":\"12.5%\"}],[\"$\",\"td\",null,{\"children\":\"18.2%\"}],[\"$\",\"td\",null,{\"children\":\"+5.7 pp\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These are the numbers from the actual evaluation runs. No cherry-picking; these are the primary benchmark results reported in the ASO paper.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"1-bit-semantic-compression\",\"children\":\"1-Bit Semantic Compression\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The quality attestations computed during ASO runs can be stored and reused. We compress them using 1-bit semantic quantization: each attestation is reduced to a single bit indicating above- or below-median quality for its group. This achieves 29.5× storage savings relative to full-precision scoring while preserving sufficient signal for weight computation.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Stored attestations form a semantic prior that can be loaded at the start of future runs, bootstrapping the PoE weighting without cold-starting. This is the ASO equivalent of a pre-trained adapter — except it requires no gradient computation to produce.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"cost-comparison\",\"children\":\"Cost Comparison\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Method\"}],[\"$\",\"th\",null,{\"children\":\"Approximate Cost\"}],[\"$\",\"th\",null,{\"children\":\"Modifies Weights\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ASO\"}],[\"$\",\"td\",null,{\"children\":\"~$18\"}],[\"$\",\"td\",null,{\"children\":\"No\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LoRA/QLoRA (7B)\"}],[\"$\",\"td\",null,{\"children\":\"$$10,000–$50,000\"}],[\"$\",\"td\",null,{\"children\":\"Yes\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Full RLHF\"}],[\"$\",\"td\",null,{\"children\":\"$$100,000+\"}],[\"$\",\"td\",null,{\"children\":\"Yes\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The $18 figure is inference compute for generating the candidate groups, computing quality scores, and running the PoE decoding. No GPU rental for training, no dataset curation, no checkpoint storage at training scale.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"what-this-does-not-replace\",\"children\":\"What This Does Not Replace\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"ASO is not a substitute for fine-tuning in every case. If you need a model to acquire genuinely new knowledge — factual updates after the training cutoff, proprietary internal terminology, task formats that fall entirely outside the base model's distribution — fine-tuning has capabilities that decode-time adaptation cannot replicate.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"ASO is optimal when the base model has the latent capability and the problem is eliciting it reliably. For code generation, reasoning, and agentic tasks where frontier models already have relevant priors, $18 of ASO compute gets you most of the way there.\"}],\"\\n\",[\"$\",\"$L18\",null,{\"level\":2,\"id\":\"implementation\",\"children\":\"Implementation\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"ASO is implemented in the \",[\"$\",\"code\",null,{\"children\":\"zoo/gym\"}],\" training platform and exposed through the Hanzo Agent SDK. The PoE ensemble runs against any OpenAI-compatible endpoint, making it provider-agnostic. Quality attestations are computed locally and stored in the 1-bit compressed format described above.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The full paper is available in \",[\"$\",\"code\",null,{\"children\":\"zoo/papers/hllm-training-free-grpo.tex\"}],\".\"]}]],\"className\":\"prose\"}]}]}],[\"$\",\"div\",null,{\"className\":\"mt-10\",\"children\":[\"$\",\"section\",null,{\"className\":\"border-t border-border p-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"p-6 lg:p-10\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-2xl font-medium mb-8\",\"children\":\"Read more\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-8\",\"children\":[[\"$\",\"$L8\",\"/blog/2026-02-03-decentralized-semantic-optimization\",{\"href\":\"/blog/2026-02-03-decentralized-semantic-optimization\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Decentralized Semantic Optimization: Byzantine-Robust Knowledge Sharing Across Agents\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"DSO achieves 15.2% improvement when 10 agents share semantic priors versus isolated operation, while remaining resilient to 20% Byzantine participants. Convergence rate: O(1/√n).\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"February 2, 2026\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2026-02-05-hamiltonian-market-maker\",{\"href\":\"/blog/2026-02-05-hamiltonian-market-maker\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Hamiltonian Market Maker: Physics-Inspired AMM with 32% Lower Impermanent Loss\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"HMM applies Hamiltonian mechanics to AMM design: symplectic integrators preserve geometric structure, conservation laws prevent MEV, and phase space dynamics produce 15.3% higher capital efficiency and 32% lower impermanent loss than Uniswap v3.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"February 4, 2026\"}]]}]]}],[\"$\",\"$L8\",\"/blog/2024-11-15-zen-science-launch\",{\"href\":\"/blog/2024-11-15-zen-science-launch\",\"className\":\"group grid grid-cols-1 lg:grid-cols-12 items-center gap-4 cursor-pointer\",\"children\":[\"$undefined\",[\"$\",\"div\",null,{\"className\":\"space-y-2 flex-1 col-span-1 lg:col-span-8\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-lg group-hover:underline underline-offset-4 font-semibold text-card-foreground group-hover:text-primary transition-colors line-clamp-2\",\"children\":\"Zen Science: AI for Research, Hypothesis Generation, and Literature Synthesis\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground text-sm line-clamp-3 group-hover:underline underline-offset-4\",\"children\":\"Zen Science is trained on a deep scientific corpus including PubMed, arXiv, and patent databases, enabling hypothesis generation, experimental design support, and cross-domain literature synthesis.\"}],[\"$\",\"time\",null,{\"className\":\"block text-xs font-medium text-muted-foreground\",\"children\":\"November 14, 2024\"}]]}]]}]]}]]}]}]}]]}],[\"$\",\"aside\",null,{\"className\":\"hidden lg:block w-[350px] flex-shrink-0 p-6 lg:p-10 bg-muted/60 dark:bg-muted/20\",\"children\":[\"$\",\"div\",null,{\"className\":\"sticky top-20 space-y-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-start gap-2\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-sm tracking-tight text-balance font-semibold\",\"children\":\"Hanzo AI\"}]}]}],[\"$\",\"div\",null,{\"className\":\"border border-border rounded-lg p-6 bg-card\",\"children\":[\"$\",\"$L1b\",null,{}]}]]}]}]]}],[\"$\",\"$L1c\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"content\":\"black\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning - Hanzo Blog\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"author\",\"href\":\"https://hanzo.blog\"}],[\"$\",\"meta\",\"3\",{\"name\":\"author\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning,ai,ml,optimization,research,grpo,paper,Hanzo AI,Blog,AI Research,Infrastructure\"}],[\"$\",\"meta\",\"5\",{\"name\":\"creator\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"6\",{\"name\":\"publisher\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"7\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"8\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"9\",{\"rel\":\"canonical\",\"href\":\"https://hanzo.blog/blog/2026-02-01-active-semantic-optimization\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://hanzo.blog/blog/2026-02-01-active-semantic-optimization\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:site_name\",\"content\":\"Hanzo Blog\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image\",\"content\":\"https://hanzo.blog/blog/2026-02-01-active-semantic-optimization/opengraph-image\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"17\",{\"property\":\"og:image:alt\",\"content\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning\"}],[\"$\",\"meta\",\"18\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"19\",{\"property\":\"article:published_time\",\"content\":\"2026-02-01\"}],[\"$\",\"meta\",\"20\",{\"property\":\"article:author\",\"content\":\"Hanzo AI\"}],[\"$\",\"meta\",\"21\",{\"property\":\"article:tag\",\"content\":\"ai\"}],[\"$\",\"meta\",\"22\",{\"property\":\"article:tag\",\"content\":\"ml\"}],[\"$\",\"meta\",\"23\",{\"property\":\"article:tag\",\"content\":\"optimization\"}],[\"$\",\"meta\",\"24\",{\"property\":\"article:tag\",\"content\":\"research\"}],[\"$\",\"meta\",\"25\",{\"property\":\"article:tag\",\"content\":\"grpo\"}],[\"$\",\"meta\",\"26\",{\"property\":\"article:tag\",\"content\":\"paper\"}],[\"$\",\"meta\",\"27\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"28\",{\"name\":\"twitter:site\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"29\",{\"name\":\"twitter:creator\",\"content\":\"@hanzoai\"}],[\"$\",\"meta\",\"30\",{\"name\":\"twitter:title\",\"content\":\"Active Semantic Optimization: $18 Adaptation vs $10,000+ Fine-Tuning\"}],[\"$\",\"meta\",\"31\",{\"name\":\"twitter:description\",\"content\":\"ASO adapts language models at decode time using Training-Free GRPO and a product-of-experts ensemble. Cost: ~$18. Results: +9.1% HumanEval, +7.2% MBPP, +5.7% SWE-bench Verified.\"}],[\"$\",\"meta\",\"32\",{\"name\":\"twitter:image\",\"content\":\"https://hanzo.blog/blog/2026-02-01-active-semantic-optimization/opengraph-image\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"16:{\"metadata\":\"$e:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>